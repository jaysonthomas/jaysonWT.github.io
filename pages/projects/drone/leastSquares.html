<!DOCTYPE html>

<html>

  <head>
    <title>Least squares</title>
    <meta name="Least squares" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="https://jaysonthomas.github.io/interviewPrep.html" />

    <script type="text/javascript" src="../../../chapters.js"></script>
    <script type="text/javascript" src="../../../notes.js"></script>
  
    <script src="../../../notes-mathjax-config.js" defer></script> 
    <script type="text/javascript" id="MathJax-script" defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  
    <link rel="stylesheet" type="text/css" href="../../../notes.css" />
  </head>

<body onload="loadChapter('');">

  <!-- The following division should be written on every page -->
<div data-type="titlepage" pdf="no">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">logbook</a></h1>
    <p data-type="subtitle">Mostly control systems</p> 
    <p style="font-size: 18px;"><a href="../../../bio/jjwt.html">Jayson Wynne-Thomas</a></p>
    <p style="font-size: 14px; text-align: right;"> 
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
    </p>
  </header>
</div>

<table style="width:100%;" pdf="no"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter"></a></td>
  <td style="width:33%;text-align:center;"><a href="discreteProb.html">Discrete Probability</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=robot.html></a></td>
</tr></table>

<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 0"><h1>Least squares</h1>

<section id="table_of_contents"><h1>Table of Contents</h1>
  <ul>
    <li><a href="#1">Overview</a></li>
    <li><a href="recursiveLSs.html">Recursive LSs</a></li>
  </ul>
</section>

<section id="1"><h1>Overview</h1>
  <code>State estimation</code> (process of computing a physical quantity like position 
  from a set of measurements) is used for localisation in autonomous driving.
  Related to state estimation is the idea of <code>parameter estimation</code>. 
  Unlike a <code>state</code>, which we will define as a physical quantity that changes over time, 
  a <code>parameter</code> is constant over time. 
  Position and orientation are states of a moving vehicle, while the resistance of a particular 
  resistor in the electrical sub-system of a vehicle would be a parameter.

  <p>
    Things to know about:
    <ul>
      <li>Ordinary and weighted least squares</li>
      <ul>
        <li>How is the LS error criterion used to estimate the best parameters</li>
        <li>Derive the necessary normal equations that we'll need to 
          solve to use the method</li>
      </ul>
      <li>Recursive least squares</li>
      <li>Link between least squares and the maximum likelihood estimation technique</li>
    </ul>
  </p>
  Carl Friedrich Gauss' description of <code>Least Squares</code>:
  <br>
  The most probable value of an unknown parameter is that which minimizes the sum of 
  squared errors between what we observe and what we expect multiplied by numbers that
  measure the degree of precision.

  <p>
    The <a href="../../fundamentals/maths.html#2">fundamental maths</a> for this chapter.
    <a href="https://t.ly/EKOi">Coursera ref</a>.
  </p>
</section>

<section id="2"><h1>Example</h1>
  Suppose we are trying to measure the value in ohms of a carbon film
  simple resistor within the drive system of an autonomous vehicle. Let's say we collect 
  4 separate measurements, sequentially using a multimeter. 
  Let's say the resistor is rated at $1k\Omega$. 
  However, due to a number of factors, the true resistance can vary from the rated value. 
  The resistor has a gold band which indicates that it can vary by as much as 
  5%. Furthermore, let's imagine that our multimeter is particularly poor and that 
  the person making the measurements is not particularly careful. 
  <p>
    <table class="table1 center">
      <tr>
        <th>Measurement</th>
        <th>Resistance $\Omega$</th>
      </tr>
      <tr>
        <td>1</td>
        <td>1068</td>
      </tr>
      <tr>
        <td>2</td>
        <td>988</td>
      </tr>
      <tr>
        <td>3</td>
        <td>1002</td>
      </tr>
      <tr>
        <td>4</td>
        <td>996</td>
      </tr>
    </table>
  </p>

  Let $x$ be the actual resistance. Assume it is a constant, but unknown.
  We make measurements, $y$, of the resistance. We model our measurements as
  corrupted by noise $v$.
  $$\text{Measurement model}: y = x+v$$

  For now, we'll treat this noise as equivalent to a general error, 
  i.e. not interpret the noise from a probabilistic perspective.
  
  <figure>
    <img style="height:150px; width:auto"
    src="../../../figures/drone/18_ls1_measModel.png"/>
  </figure>

  <p>
    For each of the four measurements, we define a scalar noise term that is independent of the 
    other noise terms. Statistically, we would say in this case that the noise is 
    <code>independent and identically distributed</code> or <code>IID</code>. 
  </p>
    
  Next, we define the error between each measurement and the actual value of our resistance $x$. 
  But remember, we don't yet know what $x$ is. To find $x$, we square these errors to 
  arrive at an equation that is a function of our measurements and the unknown resistance 
  that we're looking for. 
</section>

<section id="3"><h1>Minimising the squared error criterion</h1>
  <p>
    With the above errors defined, the method of least squares says that the resistance value we 
    are looking for, i.e. the best estimate of $x$ ($\hat{x}_{LS}$ or $\mathcal{L}_{LS}(x)$) 
    is one which minimizes the <code>squared error criterion</code>, also sometimes called the 
    <code>squared error cost function</code> or <code>loss function</code>; i.e. the sum of 
    squared errors.
  </p>
  $$
    \hat{x}_{LS} = argmin_x(e^2_1 + e^2_2 + e^2_3 + e^2_4)
  $$

  To minimize the squared error criterion, we'll rewrite our errors in matrix notation. 
  This will be especially helpful when we have to deal with hundreds or even thousands of 
  measurements. 
  
  <p></p>
  We'll define an error vector identified as bold e, that is a function of our 
  observations stacked into a vector y, a matrix H called the <code>Jacobian</code> and the 
  true resistance. 
  $$\begin{gather*}
    \mathbf{e} = \mathbf{y} - \mathbf{H}x\\
    \begin{bmatrix}e_1 \\ e_2 \\ e_3 \\ e_4\end{bmatrix}
    = \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4\end{bmatrix} -
    \begin{bmatrix}1 \\ 1 \\ 1 \\ 1\end{bmatrix}x
  \end{gather*}$$
  
  $H$ has the dimensions $m\times n$, where $m$ is the number of measurements and $n$ 
  is the number of unknowns or parameters that we wish to estimate. In general, $H$
  is a rectangular matrix that is easy to write down in this linear case but
  will require more mathematical effort to compute in the case of non-linear 
  estimation.
  
  <p>
    $x$ is a single scalar here. It can be a vector comprising multiple unknowns. 
  </p>
  We can convert our squared error criterion to vector notation as follows. 

  $$\begin{align*}
    \mathcal{L}_{LS}(x) = e^2_1 + e^2_2 + e^2_3 + e^2_4 &= \mathbf{e}^T\mathbf{e} \\
    &= (\mathbf{y} - \mathbf{H}x)^T(\mathbf{y} - \mathbf{H}x) \\
    &= \mathbf{y}^T\mathbf{y} - x^T\mathbf{H}^T\mathbf{y} - \mathbf{y}^T\mathbf{H}x
    + x^T\mathbf{H}^T\mathbf{H}x
  \end{align*}$$
  
  We need to minimize the squared error with respect to our true resistance $x$. 
  From calculus, we know that we can solve for an extremum (here, a minimum) 
  by taking the partial derivative of the function (here, squared error) w.r.t. 
  the unknown $x$ and setting the derivative to $0$. 
  
  $$\begin{align*}
    \frac{\partial L}{\partial x}\Bigr|_{x=\hat{x}} = 
    -y^TH - y^TH + 2\hat{x}^TH^TH = 0 \\
    -2y^TH + 2\hat{x}^TH^TH = 0
  \end{align*}$$

  Re-arranging, we arrive at what are called the <code>normal equations</code>, which can be 
  written as a single matrix formula. We can solve these to find $\hat{x}_{LS}$, 
  the resistance which minimizes our squared error criterion. 
    
  $$
    \hat{x}_{LS} = (H^TH)^{-1}H^Ty
  $$

  This expression has a unique solution, i.e. we will only be able to solve for $\hat{x}$ 
  if and only if $(H^TH)^{-1}$ is not singular i.e. if the matrix $(H^TH)$ has an inverse,
  i.e. if it is invertible. 
  
  If we have m measurements and n unknown parameters, then
  $$
    H \in \mathbb{R}^{m\times n} \quad\quad H^TH \in \mathbb{R}^{n\times n}
  $$
   
  The matrix exists (and therefore derive the least squares 
  solution) if and only if there are at least as many measurements as there are unknown 
  parameters. This will usually not be a problem. In fact, we'll often face the challenge of 
  dealing with too many measurements. 
  But nevertheless, keep this limitation in mind when working with the formula. 

  <p>
    For our example:
    $$
      y = \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996 \end{bmatrix}
      \quad
      H = \begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
    $$
    $\hat{x}_{LS} = (H^TH)^{-1}H^Ty$
    $$
      \begin{bmatrix}
      \begin{bmatrix}1 & 1 & 1 & 1 \end{bmatrix}
      \begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
      \end{bmatrix}^{-1}
      \begin{bmatrix}1 & 1 & 1 & 1 \end{bmatrix}
      \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996 \end{bmatrix}
      = \frac{1}{4}(1068 + 988 + 1002  996) = 1013.5 \Omega
    $$

    Note that the expression simplifies to the arithmetic mean of the 4 measurements. 
    Now, we have another justification for using the arithmetic mean, 
    it minimizes the simple least squares criterion. 
  </p>
</section>

<section id="4"><h1>Method of LS assumptions</h1>
  We've assumed that:
  <ul>
    <li>Our measurement model is linear</li>
    This is a very important assumption that is often broken in complex systems. 
    
    <p>
      <li>Our measurements have an equal weight in our error equation</li>
      We've assumed that we care about each of our measurements equally
    </p>
  </ul>
</section>

<section id="5"><h1>Weighted LS</h1>
  What if some of our measurements are of better quality than others i.e. there's
  varying measurement noise variance? For example, 1 sensor is better than the other.

  <p>
    From now on, 
    let's drop the assumption that we're only estimating one parameter and derive the 
    more general normal equations. This will allow us to formulate a way to estimate multiple 
    parameters at one time. For example, if we wanted to estimate several resistance values 
    at once. 
  </p>

  Let's begin by using the following general notation. We have $m$ 
  measurements that are related to $n$ unknown parameters through a linear model 
  represented by $H$, the Jacobian matrix whose form and entries will depend on the particular 
  problem at hand. 

  $$
    \begin{bmatrix}y_1 \\ . \\ . \\ y_m\end{bmatrix}
    = H\begin{bmatrix}x_1 \\ . \\ . \\ x_n\end{bmatrix}
    + \begin{bmatrix}v_1 \\ . \\ . \\ v_m\end{bmatrix}
  $$
  $$
    y = Hx + v
  $$
  
  One way to interpret the ordinary method of least squares is to say 
  that we are implicitly assuming that each noise term v_i is an independent random 
  variable across measurements and has an equal variance i.e. IID.

  $$
    \mathbb{E}[v^2_i] = \sigma^2\enspace (i=1,..,m)
    \qquad \qquad R = \mathbb{E}[vv^T]=
    \begin{bmatrix}
    \sigma^2 & & 0 \\
    & \ddots & \\
    0 & & \sigma^2    
    \end{bmatrix}
  $$
  
  $\mathbb{E}$ is the expected value. The expected value of the square of the deviation
  from the mean (here, the mean is assumed to be the true value) is the same as the 
  variance.
  
  <p>
    If we instead assume that each noise term is independent but has a
    different variance, we can define our noise covariance as follows.
  </p>

  $$
    \mathbb{E}[v^2_i] = \sigma^2_i \enspace (i=1,..,m)
    \qquad \qquad R = \mathbb{E}[vv^T]=
    \begin{bmatrix}
    \sigma_1^2 & & 0 \\
    & \ddots & \\
    0 & & \sigma_m^2    
    \end{bmatrix}
  $$

  We can define a weighted least squares criterion as:
  $$\begin{align*}
    \mathcal{L}_{WLS}(x) &= e^TR^{-1}e\\
    &= \frac{e^2_1}{\sigma^2_1} + \cdots + \frac{e^2_m}{\sigma^2_m}
  \end{align*}$$
  
  Each squared error term is now weighted by the inverse of the variance 
  associated with the corresponding measurement. In other words, the lower the 
  variance of the noise, the more strongly it's associated error term will 
  be weighted in the loss function. 
  
  <p>
    In the case of equal variances, the same parameters that 
    minimize our weighted least squares criterion will also minimize our ordinary least 
    squares criterion as we should expect. 
  </p>

</section>

<section id="6"><h1>Minimising the weighted LS criterion</h1>
  Expanding the new criterion:

  $$\begin{align*}
    \mathcal{L}_{WLS}(x) &= e^TR^{-1}e\\
    &= (y-Hx)^TR^{-1}(y-Hx)
  \end{align*}$$

  We minimise the same way as before by taking a derivative. 
  In the general case where we have $n$ unknown parameters in our bold vector $x$, 
  this derivative will actually be a gradient. Setting the gradient to the 0 vector, 
  we then solve for our best or optimal parameter vector $\mathbf{\hat{x}}$. 
  $$
    \hat{x} = argmin_x\mathcal{L}(x) \qquad \longrightarrow \qquad
    \frac{\partial{\mathcal L}}{\partial x}\Bigr|_{x=\hat{x}}
    = 0 = -y^TR^{-1}H + \hat{x}^TH^TR^{-1}H
  $$
  This leads to another set of normal equations called the weighted normal equations. 
  $$\begin{align*}
    H^TR^{-1}H\hat{x}_{WLS} &= H^TR^{-1}y \\
    \hat{x} &= (H^TR^{-1}H)^{-1} H^TR^{-1}y
  \end{align*}$$

  An individual variance is assigned to each measurement, which means that the 
  matrix $\mathbf{R}$ (and its inverse) is $m \times m$ in size.
</section>
  
<section id="7"><h1>Weighted LS example</h1>
  <figure>
    <img style="height:70px; width:auto"
    src="../../../figures/drone/19_wlsExample.png"/>
  </figure>

  Noise is described in standard deviations, hence why they have the units of ohms.
  They need to be squared to get the variances.

  $$\begin{align*}
    \hat{x} &= (H^TR^{-1}H)^{-1} H^TR^{-1}y \\
    &=
    \begin{bmatrix}
      \begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix} &
      \begin{bmatrix}400&&& \\ &400&& \\ &&4& \\ &&&4 \end{bmatrix}^{-1} &
      \begin{bmatrix}1 \\ 1 \\ 1 \\ 1\end{bmatrix}
    \end{bmatrix}^{-1} 

    \begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix}
    \begin{bmatrix}400&&& \\ &400&& \\ &&4& \\ &&&4 \end{bmatrix}^{-1}
    \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996\end{bmatrix} \\

    &= \frac{1}{1/400 + 1/400 + 1/4 + 1/4}
    (\frac{1068}{400} + \frac{988}{400} + \frac{1002}{4} + \frac{996}{4}) \\
    &= 999.3\Omega
  \end{align*}$$

  Evaluating the weighted least squares solution, we can see that the 
  final resistance value is much closer to what the more accurate multimeter 
  measured.
</section>

<section id="8"><h1>Summary of WLS vs LS</h1>
  By using weighted least squares, we can vary the importance of each measurement to the 
  final estimate. It's important to be comfortable working with different measurement 
  variances and also with measurements that are sometimes correlated. 

  <p>
    <table class="table2 center">
      <tr>
        <th></th>
        <th>LSs</th>
        <th>WLSs</th>
      </tr>
      <tr>
        <td>Loss/Criterion</td>
        <td>$\mathcal L_{LS}(x) = e^Te$</td>
        <td>$\mathcal L_{WLS}(x) = e^TR^{-1}e$</td>
      </tr>
      <tr>
        <td>Solution</td>
        <td>$\hat{x}_{LS} = (H^TH)^{-1}H^Ty$</td>
        <td>$\hat{x}_{WLS} = (H^TR^{-1}H)^{-1}H^TR^{-1}y$</td>
      </tr>
      <tr>
        <td>Limitations</td>
        <td>$m\geq n$</td>
        <td>
          $m \geq n$ <br>
          $\sigma^2_i > 0$
        </td>
      </tr>
    </table>
  </p>

  We need to make sure that we model our error sources correctly.
  We derived the weighted 
  least squares criterion and the associated weighted normal equations that can be solved 
  to yield the weighted least squares estimate of a set of 'constant parameters'. 
  We need to modify the method of least squares to work 
  recursively, that is to compute an optimal estimate based on a stream of measurements
  without having to acquire the entire set beforehand, when we look at state estimation, 
  or the problem of estimating quantities that change continuously over time.
</section>

<section id="9"><h1>Independent and identically distributed</h1>
  What does it mean for a set of random variables to be independent and identically distributed?
  <ul>
    <li>
      Each random variable follows the same probability distribution and the variance of 
      any random variable does not depend on the other variables.
    </li>
    Close! It is true that, if the random variables all follow the same probability 
    distribution, each must have the same variance (which is fixed). 
    However, the term "i.i.d" refers to the statistical independence of any pair of 
    random variables.
    <p>
      <li>
        Each random variable follows the same probability distribution and all the variables 
        are mutually independent (i.e., the cross-covariance of any pair is zero).
      </li>
      Right! This relationship is shown on the last slide, where the entries on the main 
      diagonal of the matrix \mathbf{R}R are the same and the off-diagonal entries are all 
      equal to zero.
    </p>
  </ul>
  






</section>

<section id="10"><h1>LS examples</h1>
  <ul>
    <li>Example 1 </li>
    V=IR. We don't know R. We collect lots of data related to V and I,
    and fit a straight line through the points. The slope gives R.
    Include reference to jlib documentation. Start jlib documentation from
    index.
    <a href="../">Test</a>
  </ul>
</section>

<section id="11"><h1>Musings</h1>
  <ul>
    <li>Ordinary LS measurement variance</li>
    If the 4 resistance measurements are taken by different sensors;
    what do we mean when we say the variances are different: 

    <p>
      For each of the 4 sensors, take 100 measurements and work out the 
      variance. Variance is the sum of deviation from the mean times the probability
      of occurrence of the corresponding measurement. The probability gives an indication
      of how repeatable the measurement is. The variance gives an indication of how 
      accurate the measurement is, but only w.r.t. the mean which itself could be inaccurate.
      I'm guessing a manufacturer will need to provide this info when they do tests of 
      measuring known physical properties.
    </p>

    <li>
      When minimising the sum of squared errors criterion (imagining a plot with $e^2$
      on y-axis vs $x$ on the x axis), how do we ensure we don't get stuck in a local
      maximum instead of the minimum when we set $dx=0$?
    </li>

    <p>
      <li>
        Just to confirm, when someone says the variance is $x$, it gives no indication
        of the accuracy right? It only gives an indication of the bias of each sensor
        from whatever the mean is. I think <a href="https://bit.ly/3VB6OnG">this post</a>
        is confirming my thoughts are correct.
      </li>
    </p>

    <li>Is sensor variance a measure of accuracy</li>
    Googling this question gave led to many interesting link in reasearch gate and
    different stack exchanges.

    <p>
      <li>
        If we're doing batch LS on $y=Hx+b$ instead of just $y=Hx$,
        does the formula change?
      </li>
      I think it's equivalent to $y=H_1x+H_2b$
    </p>

    <li>Understand the derivation of RLS</li>
    <a href="https://t.ly/tzde">This</a> might be a good derivation.
    <a href="https://t.ly/c-GU">This</a> could be interesting as well.
  </ul>
</section>

</chapter>
</body>
</html>

