<!DOCTYPE html>

<html>

  <head>
    <title>Least squares</title>
    <meta name="Least squares" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="https://jaysonthomas.github.io/interviewPrep.html" />

    <script type="text/javascript" src="../../../chapters.js"></script>
    <script type="text/javascript" src="../../../notes.js"></script>
  
    <script src="../../../notes-mathjax-config.js" defer></script> 
    <script type="text/javascript" id="MathJax-script" defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    
    <link rel="stylesheet" href="../../../htmlbook/highlight/styles/default.css">
    <script src="../../../htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>
  
    <link rel="stylesheet" type="text/css" href="../../../notes.css" />
  </head>

<body onload="loadChapter('manipulation');">

  <!-- The following division should be written on every page -->
<div data-type="titlepage" pdf="no">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Jayson's notes</a></h1>
    <p data-type="subtitle">Mostly control systems</p> 
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Jayson Thomas</a></p>
    <p style="font-size: 14px; text-align: right;"> 
      &copy; Jayson Thomas, 2020-2022<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
    </p>
  </header>
</div>

<table style="width:100%;" pdf="no"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter"></a></td>
  <td style="width:33%;text-align:center;"><a href=../../../index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=robot.html>Next Chapter</a></td>
</tr></table>

<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 0"><h1>Least squares</h1>

<section id="table_of_contents"><h1>Table of Contents</h1>
  <ul>
    <li><a href="#1">Overview</a></li>
  </ul>
</section>

<section id="1"><h1>Overview</h1>
  <code>State estimation</code> (process of computing a physical quantity like position 
  from a set of measurements) is used for localisation in autonomous driving.
  Related to state estimation is the idea of <code>parameter estimation</code>. 
  Unlike a <code>state</code>, which we will define as a physical quantity that changes over time, 
  a <code>parameter</code> is constant over time. 
  Position and orientation are states of a moving vehicle, while the resistance of a particular 
  resistor in the electrical sub-system of a vehicle would be a parameter.

  <p>
    Things to know about:
    <ul>
      <li>Ordinary and weighted least squares</li>
      <ul>
        <li>How is the LS error criterion used to estimate the best parameters</li>
        <li>Derive the necessary normal equations that we'll need to 
          solve to use the method</li>
      </ul>
      <li>Recursive least squares</li>
      <li>Link between least squares and the maximum likelihood estimation technique</li>
    </ul>
  </p>
  Carl Friedrich Gauss' description of <code>Least Squares</code>:
  <br>
  The most probable value of an unknown parameter is that which minimizes the sum of 
  squared errors between what we observe and what we expect multiplied by numbers that
  measure the degree of precision.

  <p>
    The <a href="../../fundamentals/maths.html#2">fundamental maths</a> for this chapter.
    <a href="https://t.ly/EKOi">Coursera ref</a>.
  </p>
</section>

<section id="2"><h1>Example</h1>
  Suppose we are trying to measure the value in ohms of a carbon film
  simple resistor within the drive system of an autonomous vehicle. Let's say we collect 
  4 separate measurements, sequentially using a multimeter. 
  Let's say the resistor is rated at $1k\Omega$. 
  However, due to a number of factors, the true resistance can vary from the rated value. 
  The resistor has a gold band which indicates that it can vary by as much as 
  5%. Furthermore, let's imagine that our multimeter is particularly poor and that 
  the person making the measurements is not particularly careful. 
  <p>
    <table class="table1 center">
      <tr>
        <th>Measurement</th>
        <th>Resistance $\Omega$</th>
      </tr>
      <tr>
        <td>1</td>
        <td>1068</td>
      </tr>
      <tr>
        <td>2</td>
        <td>988</td>
      </tr>
      <tr>
        <td>3</td>
        <td>1002</td>
      </tr>
      <tr>
        <td>4</td>
        <td>996</td>
      </tr>
    </table>
  </p>

  Let $x$ be the actual resistance. Assume it is a constant, but unknown.
  We make measurements, $y$, of the resistance. We model our measurements as
  corrupted by noise $v$.
  $$\text{Measurement model}: y = x+v$$

  For now, we'll treat this noise as equivalent to a general error, 
  i.e. not interpret the noise from a probabilistic perspective.
  
  <figure>
    <img style="height:150px; width:auto"
    src="../../../figures/drone/18_ls1_measModel.png"/>
  </figure>

  <p>
    For each of the four measurements, we define a scalar noise term that is independent of the 
    other noise terms. Statistically, we would say in this case that the noise is 
    <code>independent and identically distributed</code> or <code>IID</code>. 
  </p>
    
  Next, we define the error between each measurement and the actual value of our resistance $x$. 
  But remember, we don't yet know what $x$ is. To find $x$, we square these errors to 
  arrive at an equation that is a function of our measurements and the unknown resistance 
  that we're looking for. 
</section>

<section id="3"><h1>Minimising the squared error criterion</h1>
  <p>
    With the above errors defined, the method of least squares says that the resistance value we 
    are looking for, i.e. the best estimate of $x$ ($\hat{x}_{LS}$ or $\mathcal{L}_{LS}(x)$) 
    is one which minimizes the <code>squared error criterion</code>, also sometimes called the 
    <code>squared error cost function</code> or <code>loss function</code>; i.e. the sum of 
    squared errors.
  </p>
  $$
    \hat{x}_{LS} = argmin_x(e^2_1 + e^2_2 + e^2_3 + e^2_4)
  $$

  To minimize the squared error criterion, we'll rewrite our errors in matrix notation. 
  This will be especially helpful when we have to deal with hundreds or even thousands of 
  measurements. 
  
  <p></p>
  We'll define an error vector identified as bold e, that is a function of our 
  observations stacked into a vector y, a matrix H called the <code>Jacobian</code> and the 
  true resistance. 
  $$\begin{gather*}
    \mathbf{e} = \mathbf{y} - \mathbf{H}x\\
    \begin{bmatrix}e_1 \\ e_2 \\ e_3 \\ e_4\end{bmatrix}
    = \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4\end{bmatrix} -
    \begin{bmatrix}1 \\ 1 \\ 1 \\ 1\end{bmatrix}x
  \end{gather*}$$
  
  $H$ has the dimensions $m\times n$, where $m$ is the number of measurements and $n$ 
  is the number of unknowns or parameters that we wish to estimate. In general, $H$
  is a rectangular matrix that is easy to write down in this linear case but
  will require more mathematical effort to compute in the case of non-linear 
  estimation.
  
  <p>
    $x$ is a single scalar here. It can be a vector comprising multiple unknowns. 
  </p>
  We can convert our squared error criterion to vector notation as follows. 

  $$\begin{align*}
    \mathcal{L}_{LS}(x) = e^2_1 + e^2_2 + e^2_3 + e^2_4 &= \mathbf{e}^T\mathbf{e} \\
    &= (\mathbf{y} - \mathbf{H}x)^T(\mathbf{y} - \mathbf{H}x) \\
    &= \mathbf{y}^T\mathbf{y} - x^T\mathbf{H}^T\mathbf{y} - \mathbf{y}^T\mathbf{H}x
    + x^T\mathbf{H}^T\mathbf{H}x
  \end{align*}$$
  
  We need to minimize the squared error with respect to our true resistance $x$. 
  From calculus, we know that we can solve for an extremum (here, a minimum) 
  by taking the partial derivative of the function (here, squared error) w.r.t. 
  the unknown $x$ and setting the derivative to $0$. 
  
  $$\begin{align*}
    \frac{\partial L}{\partial x}\Bigr|_{x=\hat{x}} = 
    -y^TH - y^TH + 2\hat{x}^TH^TH = 0 \\
    -2y^TH + 2\hat{x}^TH^TH = 0
  \end{align*}$$

  Re-arranging, we arrive at what are called the <code>normal equations</code>, which can be 
  written as a single matrix formula. We can solve these to find $\hat{x}_{LS}$, 
  the resistance which minimizes our squared error criterion. 
    
  $$
    \hat{x}_{LS} = (H^TH)^{-1}H^Ty
  $$

  This expression has a unique solution, i.e. we will only be able to solve for $\hat{x}$ 
  if and only if $(H^TH)^{-1}$ is not singular i.e. if the matrix $(H^TH)$ has an inverse,
  i.e. if it is invertible. 
  
  If we have m measurements and n unknown parameters, then
  $$
    H \in \mathbb{R}^{m\times n} \quad\quad H^TH \in \mathbb{R}^{n\times n}
  $$
   
  The matrix exists (and therefore derive the least squares 
  solution) if and only if there are at least as many measurements as there are unknown 
  parameters. This will usually not be a problem. In fact, we'll often face the challenge of 
  dealing with too many measurements. 
  But nevertheless, keep this limitation in mind when working with the formula. 

  <p>
    For our example:
    $$
      y = \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996 \end{bmatrix}
      \quad
      H = \begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
    $$
    $\hat{x}_{LS} = (H^TH)^{-1}H^Ty$
    $$
      \begin{bmatrix}
      \begin{bmatrix}1 & 1 & 1 & 1 \end{bmatrix}
      \begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
      \end{bmatrix}^{-1}
      \begin{bmatrix}1 & 1 & 1 & 1 \end{bmatrix}
      \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996 \end{bmatrix}
      = \frac{1}{4}(1068 + 988 + 1002  996) = 1013.5 \Omega
    $$

    Note that the expression simplifies to the arithmetic mean of the 4 measurements. 
    Now, we have another justification for using the arithmetic mean, 
    it minimizes the simple least squares criterion. 
  </p>
</section>

<section id="4"><h1>Method of LS assumptions</h1>
  We've assumed that:
  <ul>
    <li>Our measurement model is linear</li>
    This is a very important assumption that is often broken in complex systems. 
    
    <p>
      <li>Our measurements have an equal weight in our error equation</li>
      We've assumed that we care about each of our measurements equally
    </p>
  </ul>
</section>

<section id="5"><h1>Weighted LS</h1>
  What if some of our measurements are of better quality than others i.e. there's
  varying measurement noise variance? For example, 1 sensor is better than the other.

  <p>
    From now on, 
    let's drop the assumption that we're only estimating one parameter and derive the 
    more general normal equations. This will allow us to formulate a way to estimate multiple 
    parameters at one time. For example, if we wanted to estimate several resistance values 
    at once. 
  </p>

  Let's begin by using the following general notation. We have $m$ 
  measurements that are related to $n$ unknown parameters through a linear model 
  represented by $H$, the Jacobian matrix whose form and entries will depend on the particular 
  problem at hand. 

  $$
    \begin{bmatrix}y_1 \\ . \\ . \\ y_m\end{bmatrix}
    = H\begin{bmatrix}x_1 \\ . \\ . \\ x_n\end{bmatrix}
    + \begin{bmatrix}v_1 \\ . \\ . \\ v_m\end{bmatrix}
  $$
  $$
    y = Hx + v
  $$
  
  One way to interpret the ordinary method of least squares is to say 
  that we are implicitly assuming that each noise term v_i is an independent random 
  variable across measurements and has an equal variance i.e. IID.

  $$
    \mathbb{E}[v^2_i] = \sigma^2\enspace (i=1,..,m)
    \qquad \qquad R = \mathbb{E}[vv^T]=
    \begin{bmatrix}
    \sigma^2 & & 0 \\
    & \ddots & \\
    0 & & \sigma^2    
    \end{bmatrix}
  $$
  
  <p>
    If we instead assume that each noise term has a
    different variance, we can define our noise covariance as follows.
  </p>
With this definition, we can define a weighted least squares criterion. By expanding this expression, we can see why we call this weighted least squares. Each squared error term is now weighted by the inverse of the variance associated with the corresponding measurement. In other words, the lower the variance of the noise, the more strongly it's associated error term will be weighted in the loss function. We care more about errors which come from low noise measurements since those should tell us a lot about the true values of our unknown parameters. Before we see how we can minimize this new weighted criterion, let's look at what happens if we set all of the noise standard deviations to the same value sigma. In this case, we can factor out the variance in the denominator. Since the sigma squared term is constant it will not affect the minimization. This means that in the case of equal variances, the same parameters that minimize our weighted least squares criterion will also minimize our ordinary least squares criterion as we should expect. Returning to our weighted least squares criterion, we approach it's minimization the same way as before, we take a derivative. In the general case where we have n unknown parameters in our bold vector x, this derivative will actually be a gradient. Setting the gradient of the zero vector, we then solve for our best or optimal parameter vector x hat. This leads to another set of normal equations this time called the weighted normal equations. Let's take a look at an example of how this method of weighted least squares works. We'll take the same data we collected before, but now assume that the last two measurements were actually taken with a multimeter that had a much smaller noise variance. Be careful here, the numbers we list are standard deviations which is why they have the units of ohms. In order to use them in our formulation, we will need to square them to get the variances. Defining our variables and then evaluating our weighted least squares solution, we can see that the final resistance value we get is much closer to what the more accurate multimeter measured as expected.

Here's a quick summary of the methods of least squares and weighted least squares. By using weighted least squares, we can vary the importance of each measurement to the final estimate. It's important to be comfortable working with different measurement variances and also with measurements that are sometimes correlated. A self-driving car will have a number of different and complex sensors on board and we need to make sure that we model our error sources correctly. So, there you have it, weighted least squares. In this video, we discussed how certain measurements may come from sensors with better noise characteristics and should therefore be weighted more heavily in our least squares criterion. Using this intuition, we derived the weighted least squares criterion and the associated weighted normal equations that can be solved to yield the weighted least squares estimate of a set of constant parameters. In the next video, we'll look at modifying the method of least squares to work recursively, that is to compute an optimal estimate based on a stream of measurements without having to acquire the entire set beforehand. This will be very important when we look at state estimation, or the problem of estimating quantities that change continuously over time.
</section>
<section id="10"><h1>Musings</h1>
  <ul>
    <li>Ordinary LS measurement variance</li>
    If the 4 resistance measurements are taken by different sensors;
    what do we mean when we say the variances are different: 

    <p>
      For each of the 4 sensors, take 100 measurements and work out the 
      variance. Variance is the sum of deviation from the mean times the probability
      of occurrence of the corresponding measurement. The probability gives an indication
      of how repeatable the measurement is. The variance gives an indication of how 
      accurate the measurement is, but only w.r.t. the mean which itself could be inaccurate.
      I'm guessing a manufacturer will need to provide this info when they do tests of 
      measuring known physical properties.
    </p>

    <li>
      When minimising the sum of squared errors criterion (imagining a plot with $e^2$
      on y-axis vs $x$ on the x axis), how do we ensure we don't get stuck in a local
      maximum instead of the minimum when we set $dx=0$?
    </li>

    <p>
      <li>
        Just to confirm, when someone says the variance is $x$, it gives no indication
        of the accuracy right? It only gives an indication of the bias of each sensor
        from whatever the mean is. I think <a href="https://bit.ly/3VB6OnG">this post</a>
        is confirming my thoughts are correct.
      </li>
    </p>

    <li>Is sensor variance a measure of accuracy</li>
    Googling this question gave led to many interesting link in reasearch gate and
    different stack exchanges.
  </ul>
</section>

</chapter>
</body>
</html>

