<!DOCTYPE html>

<html>

  <head>
    <title>Recursive least squares</title>
    <meta name="Recursive least squares" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="https://jaysonthomas.github.io/interviewPrep.html" />

    <script type="text/javascript" src="../../../chapters.js"></script>
    <script type="text/javascript" src="../../../notes.js"></script>
  
    <script src="../../../notes-mathjax-config.js" defer></script> 
    <script type="text/javascript" id="MathJax-script" defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    
    <link rel="stylesheet" href="../../../htmlbook/highlight/styles/default.css">
    <script src="../../../htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>
  
    <link rel="stylesheet" type="text/css" href="../../../notes.css" />
  </head>

<body onload="loadChapter('manipulation');">

  <!-- The following division should be written on every page -->
<div data-type="titlepage" pdf="no">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Jayson's logbook</a></h1>
    <p data-type="subtitle">Mostly control systems</p> 
    <p style="font-size: 18px;"><a href="../../../bio/jjwt.html">Jayson Wynne-Thomas</a></p>
    <p style="font-size: 14px; text-align: right;"> 
      &copy; Jayson Wynne-Thomas, 2020-2022<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
    </p>
  </header>
</div>

<table style="width:100%;" pdf="no"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter"></a></td>
  <td style="width:33%;text-align:center;"><a href=../../../index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=robot.html>Next Chapter</a></td>
</tr></table>

<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 0"><h1>Recursive least squares</h1>

<section id="table_of_contents"><h1>Table of Contents</h1>
  <ul>
    <li><a href="#1">Overview</a></li>
  </ul>
</section>

<section id="1"><h1>Overview</h1>
  RLS extends the batch LS solution to work on the fly by keeping a running estimate of the 
  LS solution as new measurements stream in. 
  
  <p>
    Ideally, we'd like to have as 
    many measurements as possible to get an accurate estimate. But, the amount of 
    computational resources needed to solve 
    our normal equations will grow with the measurement vector size. For this reason, having
    a batch of stored measurements is not a good idea.  
  </p>

  We use a recursive method to keep a running estimate 
  of the optimal parameter for all of the measurements that we've collected up to the previous 
  time step and then update the estimate given the measurement at the current time step. 
  Thus we incrementally update our estimate as we go along.
</section>

<section id="2"><h1>Linear recursive estimator</h1>
  Let us assume that we have our best optimal estimate of our unknown parameters at time $k-1$. 
  At time $k$ we receive a new measurement that we will assume follows a linear measurement 
  model with additive Gaussian noise. 

  $$
    y_k = H_kx + v_k
  $$
  
  Our goal is to compute an updated optimal estimate at 
  time $k$, given our measurement $y_k$ and the previous estimate $\hat{x}_{k-1}$. 
  <p></p>

  A linear recursive estimate is given by the following expression:
  
  $$
    \hat{x}_k = \hat{x}_{k-1} + 
    \underbrace{K_k}_{\text{Estimator} \atop \text{gain matrix}}
    \underbrace{(y_k - H_k\hat{x}_{k-1})}_{\text Innovation}
  $$
  <code>Innovation</code> quantifies how well our current measurement matches our previous 
  best estimate. 
  
  <p>
    We update our new state (estimate) as a linear combination of the previous estimate 
    (best guess) and the current measurement residual (or error, 
    difference between what we expected the measurement to be and what we actually measured),
    weighted by a gain matrix $K_k$
    If the innovation were equal to $0$, we would not change our old estimate at all. 
  </p>
</section>

<section id="3"><h1>Computing the estimator gain matrix $K_k$</h1>
  We can compute it by minimising a similar LSs criterion, but this time we'll use a
  probabilistic formulation. We wish to minimise the expected value of the sum of squared
  errors of our current estimate at time step $k$.

  <p>
    For a single scalar parameter like resistance, this amounts to minimising the estimator 
    state variance:
  </p>
  $$\begin{align*}
    \mathcal L_{RLS} &= \mathbb{E}[(x_k - \hat{x}_k)^2] \\
    &= \sigma^2_k
  \end{align*}$$

  For multiple $n$ unknown parameters, this is equivalent 
  to minimising the trace of our state covariance matrix at time step $k$: 
  
  $$\begin{align*}
    \mathcal L_{RLS} &= \mathbb{E}[(x_{1k} - \hat{x}_{1k})^2
    + \cdots + (x_{nk} - \hat{x}_{nk})^2] \\
    &= Trace(\underbrace{P_k}_{\text Estimator \atop \text covariance})
  \end{align*}$$
  
  Instead of minimising the error directly, we minimise its expected value which is actually 
  the estimator variance. The lower the variance, the more we are certain of our estimate. 
  
  <p>
    Using our linear recursive formulation, we can express the state covariance matrix $P_k$
    as a function of $K_k$:

    $$
      P_k = (1-K_kH_k)P_{k-1}(1-K_kH_k)^T + K_kR_kK^T_k
    $$  
  </p>
  
  By using matrix calculus and taking derivatives, we can show that this criterion is minimised 
  when $K_k$ has the following value: (find the full derivation in any standard estimation text)

  $$
    K_k = P_{k-1}H^T_k(H_kP_{k-1}H^T_k + R_k)^{-1}
  $$
  
  Finally, by using this formulation, we can also rewrite our recursive definition for 
  $P_k$ into something much simpler. 

  $$\begin{align*}
    P_k &= P_{k-1} - K_kH_kP_{k-1} \\
    &= (1-K_kH_k)P_{k-1}
  \end{align*}$$
  
  The larger the gain matrix $KÂ£, the smaller the new estimator covariance will be. 
  Intuitively, the gain matrix balances the information we get from the 
  prior estimate and the information we receive from the new measurement. 
</section>

<section id="4"><h1>RLS algorithm</h1>
  <ul>
    <li>Initialise the estimator</li>
    Initialise the estimate of the unknown parameters and the corresponding covariance 
    matrices.

    $$\begin{align*}
      \hat{x}_0 &= \mathbb{E}[x] \\
      P_0 &= \mathbb{E}[(x-\hat{x}_0)(x-\hat{x}_0)^T]
    \end{align*}$$
  </ul>
  This initial guess could come from the first measurement we take and the 
  covariance could come from technical specifications. 
  
  <p>
    <li>
      Set up the measurement model, defining the Jacobian and the measurement 
      covariance matrix
    </li>
    $$
      y_k = H_kx + v_k
    $$
  </p>

  <li>Update the estimate $\hat{x}_k$ and the covariance $P_k$</li>
  Using the following:
  $$\begin{align*}
    K_k &= P_{k-1}H^T_k(H_kP_{k-1}H^T_k + R_k)^{-1} \\
    \hat{x}_k &= \hat{x}_{k-1} + K_k(y_k - H_k\hat{x}_{k-1}) \\
    P_k &= (1-K_kH_k)P_{k-1}
  \end{align*}$$
  
  Every time a measurement is recorded, we compute the measurement gain and then 
  use it to update the estimate of the parameters and the estimator covariance or uncertainty. 
  Every time we get a new measurement our parameter uncertainty shrinks. 
</section>

<section id="5"><h1>Summary</h1>
  RLS enables us to 
  minimise computational effort in our estimation process. More 
  importantly, it forms the update step of the linear Kalman filter. 
  RLS lets us produce a running estimate of a 
  parameter without having to have the entire batch of measurements at hand. It 
  is a recursive linear estimator that minimises the variance of the parameters 
  at the current time. 

  <p>
    In the jupyter notebook, use RLS to determine a voltage value from a series of measurements. 
  </p>
</section>

</chapter>
</body>
</html>