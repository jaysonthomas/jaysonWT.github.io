
<!DOCTYPE html>
<html>
<head>
  <title>Multivariate distributions</title>
  <meta name="Multivariate distributions" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>

  <table style="width:100%;" pdf="no"><tr style="width:100%">
    <td style="width:33%;text-align:left;">
      <a class="previous_chapter" href="">Prev Chapter</a>
    </td>
    
    <td style="width:33%;text-align:center;">
      <a href="">Root Chapter</a>
    </td>
    
    <td style="width:33%;text-align:right;">
      <a class="next_chapter" href="">Next Chapter</a>
    </td>
  </tr></table>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Intro</a>
<a href="#1">Vector value functions</a>
<a href="#2">Multivariate Gaussian and its parameter estimates</a>
<a href="#3">Example of a two dimensional covariance</a>
<a href="#4">Musings</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Multivariate distributions</h1>

<section id="0"><h1>Intro</h1>
  What if our random variable is a vector $x=[x_1,x_2,..,x_n]$, and not a scalar.
  The vehicle position isn't going to be given by a single variable x, but by
  x, y and z. And if we include attitude as well, $x$ becomes a six dimensional vector.
  And we'd like to have a distribution over these variables.
  
  <p>
    If we treat each one separately, then we're making some strong assumptions about
    independence. But, we can actually define vector value functions and the 
    same concepts that we saw for scalar values over the vector value inputs.
  </p>
</section>

<section id="1"><h1>Vector value functions</h1>
  <figure>
    <img style="height:200px; width:80%"
    src="../../../figures/drone/15_vectorValueFunctions.png">
  </figure>
  <ul>
    <li>Normalisation</li>
    The integral still has to come to one. In this case, the normalization constant 
    that makes this come to 1 would be the integral of f(x), dx1, dx2 up to dxn.

    <p>
      <li>Mean</li>
      The mean is given by the same set of integrals,
      but it's the integral of x times f(x).
    </p>

    <li>Spread</li>
    Instead of a variance, we have a <code>covariance</code>,
    $\Sigma$, and that's going to be equal to the integral of x minus mu,
    times x minus mu transpose, times the density.
    
    <p>
      This covariance is going to be a matrix,
      because the outer product, $(x-\mu)(x-\mu)^T$, is going to result in a matrix as well.
      The covariance is going to be a <code>symmetric matrix</code> and 
      <code>positive semi definite</code> (i.e. there are no negative eigenvalues).
    </p>
  </ul>
</section>
  
<section id="2"><h1>Multivariate Gaussian and its parameter estimates</h1>
  <figure>
    <img style="height:50%; width:40%"
    src="../../../figures/drone/16_multivariateGaussian.png">
  </figure>

  The functional form is not very different from the scalar gaussian:
  <ul>
    <li>
      We're no longer taking square root of two pi, it's n over two for a vector x of length n
    </li>

    <p>
      <li>$|\Sigma|$ is the <code>determinant</code> of the covariance</li>
    </p>

    <li>
      And then we have the vector multiplication $(x-\mu)^T\Sigma^{-1}(x-\mu)$
    </li>
  </ul>
  
  The estimate of the parameters is almost the same as before,
  taking care to do the vector math correctly.
</section>

<section id="3"><h1>Example of a two dimensional covariance</h1>
  <figure>
    <img style="height:30%; width:40%"
    src="../../../figures/drone/17_multivariateGaussianExample.png">
  </figure>

  An important property of the covariance is that the 
  <code>eigenvalues</code> and <code>eigenvectors</code> of the covariance 
  describe the amount and direction of uncertainty.
  
  <p>
    Here, we have an overhead view and
    that location at the center of the ellipse would be the mean.
    If we take the covariance and extract the eigenvalues and eigenvectors,
    this would be the first eigenvector and this will be the second eigenvector.
  </p>

  If I draw an ellipse at one eigenvalue away from the mean along each vector,
  then this one eigenvalue ellipse is going to define
  a confidence region which in this case is an ellipsoid.

  <p>
    Just as one standard deviation distance away from the mean in the scalar case 
    defined the 68% confidence interval for x, the ellipse in 2D defines the confidence interval.
    The total probability inside the ellipse at 1 standard deviation is not 68% in this case.
    The total probability for a given length of
    axis actually depends on the number of variables in the random vector.
    But in 2D, 95% probability is captured by axis with length that is roughly 
    five times the eigenvalues.
  </p>
</section>  

<section id="4"><h1>Musings</h1>
  <ul>

  </ul>
</section>

</chapter>

</body>
</html>
