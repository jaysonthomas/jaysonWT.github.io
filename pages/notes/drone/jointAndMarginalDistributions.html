
<!DOCTYPE html>
<html>
<head>
  <title>Joint and Marginal distributions</title>
  <meta name="Joint and Marginal distributions" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>

  <table style="width:100%;" pdf="no"><tr style="width:100%">
    <td style="width:33%;text-align:left;">
      <a class="previous_chapter" href="">Prev Chapter</a>
    </td>
    
    <td style="width:33%;text-align:center;">
      <a href="">Root Chapter</a>
    </td>
    
    <td style="width:33%;text-align:right;">
      <a class="next_chapter" href="">Next Chapter</a>
    </td>
  </tr></table>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Definitions</a>
<ul class="no-bullets">
  <li><a href="#0.0">Marginal</a></li>
  <li><a href="#0.1">Independence</a></li>
</ul>
<a href="#1">Musings</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Joint and Marginal distributions</h1>

<section id="0"><h1>Definitions</h1>
  A lot of the times, knowing the value of one field in our vector tells us something about the other fields. For instance, if a vehicle has a distribution over a state x, y, z, knowing the altitude of the vehicle may tell us something about its lateral position or vice versa. We can't treat those probabilities separately and differently.

  <p>
    Hence, if we have a multivariate distribution, we can't just assume that we have a separate probability distribution around each variable or field in the vector.
  </p>
  
  We often have joint distributions, where if we have a vector $x = [x_1,x_2]$, then $ p(x) = p(x_1, x_2)$.
  
  <p>
    The function $p(x)$ is a probability distribution over the 2 variables that takes the vector as an input and outputs a scalar density.
  </p>

  <subsection id="0.0"><h1>Marginal</h1>
    To find the probability over $x_1$ from the joint distribution, we compute the <i>marginal</i> by integrating $x_2$ out of the density function:
    $$
      p(x_1) = \int{p(x_1,x_2)dx_2}
    $$

    If we have many variables in our vector, we can integrate out all the variables we don't want, to get a marginal over any subset of variables we do want.
  </subsection>

  <subsection id="0.1"><h1>Independence</h1>
    The system that we're estimating may have more than two variables. A flying vehicle may have variables for x, y, z, roll, pitch, yaw and perhaps even for their derivatives which takes it to 12. We might have variables that track what's happening in the environment - example, states (12 each) of other drones. This distribution can get unwieldy very quickly.
  
    <p>
      But only some of the variables may actually have information about each other, i.e. some of the random variables in the vector are independent. <i>Independence</i> is the property that the joint probability density function is the product of individual densities of the component variables:
      $$
        p(x_1,x_2) = p(x_1)p(x_2)
      $$
    </p>
    <div class="container">
      <figure>
        <img style="height:80px; width:auto"
        src="../../../figures/drone/20_notIndependent.png"/>
        <figcaption>
          Dependent
        </figcaption>
      </figure>

      <figure>
        <img style="height:80px; width:auto"
        src="../../../figures/drone/21_independent.png"/>
        <figcaption>
          Independent
        </figcaption>
      </figure>
    </div>

    We can see the effect for a 2D Gaussian where, if $x_1$ and $x_2$ are correlated, then we may have a distribution that is oval shaped and tilted. Here, if we have a positive $x_1$,
    then we're very likely to have a positive $x_2$. If we have a negative $x_1$ we're very likely to have a negative $x_2$.
    
    <p>
      On the other hand, for a spherical Gaussian, knowing where we are on the $x_1$ axis, tells us very little about where we might be on the $x_2$ axis. 
    </p>

    The idea of independence is that knowing the value of one variable has no effect on the other random variable, i.e. knowing the value of $x_1$ tells us nothing about $x_2$.
  </subsection>
</section>

<section id="1"><h1>Musings</h1>
  <ul>
    <li>If we have many variables in our vector, we can integrate out all the variables we don't want to get a marginal over any subset of variables we do want.</li>
  </ul>
</section>
</chapter>

</body>
</html>
