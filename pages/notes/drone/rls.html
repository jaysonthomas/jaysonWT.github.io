
<!DOCTYPE html>
<html>
<head>
  <title>Recursive least squares</title>
  <meta name="Recursive least squares" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>

  <table style="width:100%;" pdf="no"><tr style="width:100%">
    <td style="width:33%;text-align:left;">
      <a class="previous_chapter" href="">Prev Chapter</a>
    </td>
    
    <td style="width:33%;text-align:center;">
      <a href="">Root Chapter</a>
    </td>
    
    <td style="width:33%;text-align:right;">
      <a class="next_chapter" href="">Next Chapter</a>
    </td>
  </tr></table>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Overview</a>
<ul class="no-bullets">
  <li><a href="#0.0">Advantages</a></li>
  <li><a href="#0.1">In mathematical form</a></li>
</ul>
<a href="#1">Computing the estimator gain matrix $K_k$</a>
<a href="#2">RLS algorithm</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Recursive least squares</h1>

<section id="0"><h1>Overview</h1>
  <subsection id="0.0"><h1>Advantages</h1>
    <p>
      The more measurements we have, the more accurate we can get the estimate to be. But, the amount of computational resources needed to solve the normal equations will grow with the measurement vector size. For this reason, having a batch of stored measurements is not a good idea.  
    </p>

    RLS extends the batch LS solution to work on the fly by keeping a running estimate of the optimal parameter (the LS solution) for all measurements collected up to the previous time step and then updates the estimate given the measurement at the current time step. Thus the estimate is updated incrementally as new measurements stream in. 

    <p>
      RLS enables us to minimise computational effort in our estimation process. More importantly, it forms the update step of the linear Kalman filter. It is a recursive linear estimator that minimises the variance of the parameters at the current time. 
    </p>
  </subsection>
  
  <subsection id="0.1"><h1>In mathematical form</h1>
    Let us assume that we have our best optimal estimate of our unknown parameters at time $k-1$. At time $k$ we receive a new measurement that we will assume follows a linear measurement model with additive Gaussian noise. 
  
    $$
      \tilde{y}_k = H_kx + v_k
    $$
    
    The goal is to compute an updated optimal estimate at time $k$, given our measurement $\tilde{y}_k$ and the previous estimate $\hat{x}_{k-1}$. A linear recursive estimate is given by the following expression:
    
    $$\begin{equation} \label{eq:linearRecursiveFormulation}
      \hat{x}_k = \hat{x}_{k-1} + 
      \underbrace{K_k}_{\text{Estimator} \atop \text{gain matrix}}
      \underbrace{(\tilde{y}_k - H_k\hat{x}_{k-1})}_{\text Innovation}
      \end{equation}$$
    <i>Innovation</i> quantifies how well our current measurement matches our previous best estimate. 
    
    <p>
      We update our new state (estimate) as a linear combination of the previous estimate (<n>best guess</n>) and the current measurement <n>residual</n> (or error, difference between what we expected the measurement to be and what was actually measured), <n>weighted</n> by a gain matrix $K_k$. If the innovation were equal to $0$, we would not change our old estimate at all. 
    </p>
  </subsection>
</section>

<section id="1"><h1>Computing the estimator gain matrix $K_k$</h1>
  Instead of minimising the error directly, we consider a probabilistic formulation - we minimise the expected value of the sum of squared errors of our current estimate at time step $k$ which is actually the estimator variance. The lower the variance, the more we are certain of our estimate.

  <p>
    For a single scalar parameter like resistance, the variance is as follows:
  </p>
  $$\begin{align*}
    \mathcal L_{RLS} &= \mathbb{E}[(x_k - \hat{x}_k)^2] \\
    &= \sigma^2_k
  \end{align*}$$

  For multiple $n$ unknown parameters, this is equivalent 
  to minimising the trace of the covariance matrix at time step $k$: 
  
  $$\begin{align*}
    \mathcal L_{RLS} &= \mathbb{E}[(x_{1k} - \hat{x}_{1k})^2
    + \cdots + (x_{nk} - \hat{x}_{nk})^2] \\
    &= Trace(\underbrace{P_k}_{\text Estimator \atop \text covariance})
  \end{align*}$$
  
  <p>
    Using our linear recursive formulation (\ref{eq:linearRecursiveFormulation}), we can express the covariance matrix $P_k$ as a function of $K_k$:

    $$
      P_k = (1-K_kH_k)P_{k-1}(1-K_kH_k)^T + K_kR_kK^T_k
    $$  
  </p>
  
  By using matrix calculus and taking derivatives, we can show that this criterion is minimised when $K_k$ has the following value: (find the full derivation in any standard estimation text - <a href="https://t.ly/tzde">Ref</a>)

  $$
    K_k = P_{k-1}H^T_k(H_kP_{k-1}H^T_k + R_k)^{-1}
  $$
  
  Finally, by using this formulation, we can simplify the recursive definition for $P_k$: 

  $$\begin{align*}
    P_k &= P_{k-1} - K_kH_kP_{k-1} \\
    &= (I-K_kH_k)P_{k-1}
  \end{align*}$$
  
  The larger the gain matrix $K$, the smaller the new estimator covariance will be. 
  Intuitively, the gain matrix balances the information we get from the 
  prior estimate and the information we receive from the new measurement. 
</section>

<section id="2"><h1>RLS algorithm</h1>
  <ul>
    <li>Initialise the estimator</li>
    Initialise the estimate of the unknown parameters and the corresponding covariance 
    matrices.

    $$\begin{align*}
      \hat{x}_0 &= \mathbb{E}[x] \\
      P_0 &= \mathbb{E}[(x-\hat{x}_0)(x-\hat{x}_0)^T]
    \end{align*}$$
  
    This initial guess could come from the first measurement we take and the covariance could come from technical specifications. 
  
    <p>
      <li>
        Set up the measurement model, defining the Jacobian and the measurement covariance matrix
      </li>
      $$
        \tilde{y}_k = H_kx + v_k
      $$
    </p>

    <li>Update the estimate $\hat{x}_k$ and the covariance $P_k$</li>
    For every measurement $k$:
    $$\begin{align*}
      K_k &= P_{k-1}H^T_k(H_kP_{k-1}H^T_k + R_k)^{-1} \\
      \hat{x}_k &= \hat{x}_{k-1} + K_k(\tilde{y}_k - H_k\hat{x}_{k-1}) \\
      P_k &= (I-K_kH_k)P_{k-1}
    \end{align*}$$
    
    Every time a measurement $\tilde{y}_k$ is recorded, we compute the measurement gain $K_k$ and then use it to update the estimate and the estimator covariance or uncertainty $P_k$. Every time a new measurement is taken, the uncertainty should shrink. 
  </ul>
</section>

</chapter>

</body>
</html>
