
<!DOCTYPE html>
<html>
<head>
  <title>Least squares</title>
  <meta name="Least squares" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>

  <table style="width:100%;" pdf="no"><tr style="width:100%">
    <td style="width:33%;text-align:left;">
      <a class="previous_chapter" href="">Prev Chapter</a>
    </td>
    
    <td style="width:33%;text-align:center;">
      <a href="">Root Chapter</a>
    </td>
    
    <td style="width:33%;text-align:right;">
      <a class="next_chapter" href="">Next Chapter</a>
    </td>
  </tr></table>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Definitions</a>
<a href="#1">Assumptions in LS</a>
<a href="#2">Least squares method overview</a>
<a href="#3">Batch LS: Minimising the squared error criterion</a>
<ul class="no-bullets">
  <li><a href="#3.0">Define the overall measurement model</a></li>
  <li><a href="#3.1">Define the noise model</a></li>
  <li><a href="#3.2">Define the cost function</a></li>
  <li><a href="#3.3">Rewrite the cost function in matrix notation</a></li>
  <li><a href="#3.4">Minimise the cost function to find the estimate</a></li>
  <li><a href="#3.5">Limitation of the batch LS equation</a></li>
  <li><a href="#3.6">Example</a></li>
</ul>
<a href="#4">Weighted LS</a>
<ul class="no-bullets">
  <li><a href="#4.0">Estimating multiple unknowns</a></li>
  <li><a href="#4.1">Defining the cost function</a></li>
  <li><a href="#4.2">Minimising the cost function</a></li>
  <li><a href="#4.3">Example</a></li>
</ul>
<a href="#5">Summary of WLS vs LS</a>
<a href="#6">Independent and identically distributed</a>
<a href="#7">Musings</a>
<a href="#8">Deriving The Maximum Likelihood Estimator</a>
<ul class="no-bullets">
  <li><a href="#8.0">Breaking down the derivation steps</a></li>
</ul>
<a href="#9">Fitting A Line With Linear Least Squares</a>
<ul class="no-bullets">
  <li><a href="#9.0">Example</a></li>
</ul>
<a href="#10">The problem with non-linearities</a>
<a href="#11">Calculating the Jacobian</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Linear Least Squares</h1>
<section id="2"><h1>Overview</h1>
  <p>
    A linear function of $x$ can be written as: $y = Ax + b$. Consider a system where we take measurements of a state (which is constant at the time of measurement) or a parameter, $x$, such that:
    $$
      \underbrace{\tilde{y}}_{\scriptsize\text{Known}} = 
      \underbrace{H}_{\scriptsize\text{Known} \atop \scriptsize\text{Constant}}
      \underbrace{x}_{\scriptsize\text{Unknown} \atop \scriptsize\text{Constant}} 
      + \underbrace{v}_{\scriptsize\text{Unknown}}
    $$
  </p>

  <ul>
    <li>$x$ is the constant unknown state/parameter vector of length $n$ that is to be estimated.</li>

    <p>
      <li>$\tilde{y}$ is the vector of measurements of length $m$ that is actually received. The measurements are a linear function of the physical quantity perturbed by some unknown noise.</li>
    </p>

    <li>$v$ is the unknown error vector. The noise is assumed to be drawn from a Gaussian distribution with a $0$ mean and some covariance $R$, i.e. $v$ can be assumed to be some known variance times the identity matrix.</li>

    <p>
      <li>$H$ is the <i>measurement matrix</i>. It is assumed to be known. It remains constant when a measurement is being taken. It projects the state/parameter into the measurement space.</li>
    </p>
  </ul>
  
  <p>
    If the error $v$ is known, we can solve for $x$: $Hx = \tilde{y} - v$. Or if $x$ is known, we can solve for $v$. We don't know either and that's what makes this an estimation problem. We use a <i>maximum likelihood estimation</i> approach. <n>Intuitively, we look at the measurements and ask what state/parameter is the most likely to yield the particular set of measurements</n>.
  </p>  
</section>

<section id="8"><h1>Deriving The Maximum Likelihood Estimator</h1>
  We want to know what state is most consistent with our measurements. Mathematically, the goal is to select our estimate $\hat{x}$ to be the value of $x$ that maximizes the probability of our actual measurements $\tilde{y}$:
  <br>
  Goal: Choose $\hat{x}$ to maximise $p(y=\tilde{y} | x = \hat{x})$

  <p>
    There are two steps to this process:
    <ul>
      <li>Find $p(y|x)$</li>
      The probability density function $p(y|x)$ in this model, tells us the likelihood of the measurement y when the state is x .

      <p>
        <li>Choose $\hat{x}$</li>
        Select the estimate $\hat{x}$ to be the value that yields a maximum likelihood of the actual measurements $\tilde{y}$ given the estimate $\hat{x}$.
      </p>
    </ul>
  </p>

  <subsection id="8.0"><h1>Breaking down the derivation steps</h1>

  </subsection>
    We're going to assume that the distribution $p(y|x)$ is Gaussian. We need an expression for the parameters of the distribution:

    <ul>
      <li>Mean</li>
      Since $y = Hx + v$ and $v$ is a zero-mean gaussian, it's easy to show that the expected value, the mean, is: $\bar{y} = Hx$

      <p>
        <li>Covariance</li>
        Given the properties of our noise model, $v \sim \mathcal{N}(0,R)$, it's also easy to show that the covariance $(y-\bar{y})(y-\bar{y})^T$ is going to be equal to the covariance of just the noise, $R$.
      </p>
    </ul>

    Thus, $p(y|x) = \mathcal{N}(Hx, R)$

    <p>
      We just need to find a value of $\hat{x}$ that maximizes the likelihood of the actual measurements $\tilde{y}$ given $\hat{x}$. Writing out the formula for the above Gaussian:
    </p>

    $$
      p(\tilde{y}|x) = \underbrace{
          \frac{1}{(2\pi)^{\frac{n}{2}}\lvert{R}\rvert^{\frac{1}{2}}}
        }_{\text{Normalizer}}
      e^
        \underbrace{
          {-\frac{1}{2}(\tilde{y}-Hx)^TR^{-1}(\tilde{y}-Hx)}
        }_{\text{Term to maximise}}
    $$

    The normalizer term does not depend on $x$ at all. For the likelihood of the received measurements of $\tilde{y}$ given $x$, the specific choice of $x$ has no impact on the normalizer and can be ignored.

    <p>
      To maximise this overall function, we need to maximise the term that's in the exponent; which means we just need to maximize the function in the exponent.
    </p>


    A standard way to solve for the maximum of a function is to take the derivative and find where it is zero. If we take the derivative of the exponent, set it equal to zero, and then solve for $x$, we get:
    $$
      \hat{x} = (H^TH)^{-1}H^T\tilde{y}
    $$

    This is a solution to the least squares estimation problem given some measurements $\tilde{y}$ that are linearly related to some unknown quantity $x$ by the matrix $H$ that we assume we know. Using this, we can solve for the most likely estimate, $\hat{x}$.
</section>

<section id="9"><h1>Fitting A Line With Linear Least Squares</h1>
  [<a href="https://t.ly/mwtO">Good quick tutorial</a>].
  <br>
  We now have the equation for the maximum likelihood estimator $\hat{x}$. The best estimate is purely a function of the measurement model $H$ and the actual measurements $\tilde{y}$.

  <subsection id="9.0"><h1>Example</h1>
    We're going to fit some noisy data to a straight line, given by the equation:
    $$
      y = at + b
    $$

    <div class="container">
      <figure>
        <img style="height:125px; width:auto"
        src="../../../figures/drone/26_leastSquareLineFit.png"/>
        <figcaption>
          Straight line representing noisy data
        </figcaption>
      </figure>
    </div>
  </subsection>

Let's assume that our data is sampled from the line with some added noise. What we actually want are the properties of the line, which form our unknown state vector $x$:
$$
  x = \begin{bmatrix}
  a \\ b
  \end{bmatrix}
$$

The model assumes that our measurements $y_1, y_2, \cdots$ are drawn according to a linear function $y_i$:
$$
  y_i = at_i + b + v_i
$$
It's the unknown $v_i$ that's perturbing the measurements from the line. The measurements are in the form of:
$$
  \tilde{y} = Hx + v
$$

We have the state vector $x$ and all the measured values: 
$$\begin{bmatrix}
  \tilde{y_1} \\
  \vdots \\
  \tilde{y_m}
\end{bmatrix}$$

In order to get the vector $y_i$ to match up with the vector $\tilde{y}$, we need an $H$ that looks as follows:
$$\begin{bmatrix}
  t_1 & 1 \\
  \vdots & \vdots \\
  t_m & 1
\end{bmatrix}$$

Now that we have our measurements, we have our $H$, we can use the equation for the maximum likelihood estimate to find the best estimate for x, $\hat{x}$. 

<p>
  An important thing to note is that once we have the measurements $\tilde{y}$, the least squares estimation problem boils down to figuring out exactly what the measurement model $H$ looks like. Figuring out how to form $H$ for any estimation problem is often the tricky bit.
</p>
</section>
</chapter>

</body>
</html>
