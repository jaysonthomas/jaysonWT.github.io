
<!DOCTYPE html>
<html>
<head>
  <title>Least squares</title>
  <meta name="Least squares" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>

  <table style="width:100%;" pdf="no"><tr style="width:100%">
    <td style="width:33%;text-align:left;">
      <a class="previous_chapter" href="">Prev Chapter</a>
    </td>
    
    <td style="width:33%;text-align:center;">
      <a href="">Root Chapter</a>
    </td>
    
    <td style="width:33%;text-align:right;">
      <a class="next_chapter" href="">Next Chapter</a>
    </td>
  </tr></table>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Overview</a>
<a href="#1">Example</a>
<a href="#2">Minimising the squared error criterion</a>
<a href="#3">Method of LS assumptions</a>
<a href="#4">Weighted LS</a>
<a href="#5">Minimising the weighted LS criterion</a>
<a href="#6">Weighted LS example</a>
<a href="#7">Summary of WLS vs LS</a>
<a href="#8">Independent and identically distributed</a>
<a href="#9">LS examples</a>
<a href="#10">Musings</a>
<a href="#11">Another intro</a>
<ul class="no-bullets">
  <li><a href="#11.0">Example</a></li>
</ul>
<a href="#12">Deriving The Maximum Likelihood Estimator</a>
<ul class="no-bullets">
  <li><a href="#12.0">Breaking down the derivation steps</a></li>
</ul>
<a href="#13">Fitting A Line With Linear Least Squares</a>
<ul class="no-bullets">
  <li><a href="#13.0">Example</a></li>
</ul>
<a href="#14">The problem with non-linearities</a>
<a href="#15">Calculating the Jacobian</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Least squares</h1>

<section id="0"><h1>Overview</h1>
  <code>State estimation</code> (process of computing a physical quantity like position 
  from a set of measurements) is used for localisation in autonomous driving.
  Related to state estimation is the idea of <code>parameter estimation</code>. 
  Unlike a <code>state</code>, which we will define as a physical quantity that changes over time, 
  a <code>parameter</code> is constant over time. 
  Position and orientation are states of a moving vehicle, while the resistance of a particular 
  resistor in the electrical sub-system of a vehicle would be a parameter.

  <p>
    Things to know about:
    <ul>
      <li>Ordinary and weighted least squares</li>
      <ul>
        <li>How is the LS error criterion used to estimate the best parameters</li>
        <li>Derive the necessary normal equations that we'll need to 
          solve to use the method</li>
      </ul>
      <li>Recursive least squares</li>
      <li>Link between least squares and the maximum likelihood estimation technique</li>
    </ul>
  </p>
  Carl Friedrich Gauss' description of <code>Least Squares</code>:
  <br>
  The most probable value of an unknown parameter is that which minimizes the sum of 
  squared errors between what we observe and what we expect multiplied by numbers that
  measure the degree of precision.

  <p>
    The <a href="../../fundamentals/maths.html">fundamental maths</a> for this chapter.
    <a href="https://t.ly/EKOi">Coursera ref</a>.
  </p>
</section>

<section id="1"><h1>Example</h1>
  Suppose we are trying to measure the value in ohms of a carbon film
  simple resistor within the drive system of an autonomous vehicle. Let's say we collect 
  4 separate measurements, sequentially using a multimeter. 
  Let's say the resistor is rated at $1k\Omega$. 
  However, due to a number of factors, the true resistance can vary from the rated value. 
  The resistor has a gold band which indicates that it can vary by as much as 
  5%. Furthermore, let's imagine that our multimeter is particularly poor and that 
  the person making the measurements is not particularly careful. 
  <p>
    <table class="table1 center">
      <tr>
        <th>Measurement</th>
        <th>Resistance $\Omega$</th>
      </tr>
      <tr>
        <td>1</td>
        <td>1068</td>
      </tr>
      <tr>
        <td>2</td>
        <td>988</td>
      </tr>
      <tr>
        <td>3</td>
        <td>1002</td>
      </tr>
      <tr>
        <td>4</td>
        <td>996</td>
      </tr>
    </table>
  </p>

  Let $x$ be the actual resistance. Assume it is a constant, but unknown.
  We make measurements, $y$, of the resistance. We model our measurements as
  corrupted by noise $v$.
  $$\text{Measurement model}: y = x+v$$

  For now, we'll treat this noise as equivalent to a general error, 
  i.e. not interpret the noise from a probabilistic perspective.
  
  <figure>
    <img style="height:150px; width:auto"
    src="../../../figures/drone/18_ls1_measModel.png"/>
  </figure>

  <p>
    For each of the four measurements, we define a scalar noise term that is independent of the 
    other noise terms. Statistically, we would say in this case that the noise is 
    <code>independent and identically distributed</code> or <code>IID</code>. 
  </p>
    
  Next, we define the error between each measurement and the actual value of our resistance $x$. 
  But remember, we don't yet know what $x$ is. To find $x$, we square these errors to 
  arrive at an equation that is a function of our measurements and the unknown resistance 
  that we're looking for. 
</section>

<section id="2"><h1>Minimising the squared error criterion</h1>
  <p>
    With the above errors defined, the method of least squares says that the resistance value we 
    are looking for, i.e. the best estimate of $x$ ($\hat{x}_{LS}$ or $\mathcal{L}_{LS}(x)$) 
    is one which minimizes the <code>squared error criterion</code>, also sometimes called the 
    <code>squared error cost function</code> or <code>loss function</code>; i.e. the sum of 
    squared errors.
  </p>
  $$
    \hat{x}_{LS} = argmin_x(e^2_1 + e^2_2 + e^2_3 + e^2_4)
  $$

  To minimize the squared error criterion, we'll rewrite our errors in matrix notation. 
  This will be especially helpful when we have to deal with hundreds or even thousands of 
  measurements. 
  
  <p></p>
  We'll define an error vector identified as bold e, that is a function of our 
  observations stacked into a vector y, a matrix H called the <code>Jacobian</code> and the 
  true resistance. 
  $$\begin{gather*}
    \mathbf{e} = \mathbf{y} - \mathbf{H}x\\
    \begin{bmatrix}e_1 \\ e_2 \\ e_3 \\ e_4\end{bmatrix}
    = \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4\end{bmatrix} -
    \begin{bmatrix}1 \\ 1 \\ 1 \\ 1\end{bmatrix}x
  \end{gather*}$$
  
  $H$ has the dimensions $m\times n$, where $m$ is the number of measurements and $n$ 
  is the number of unknowns or parameters that we wish to estimate. In general, $H$
  is a rectangular matrix that is easy to write down in this linear case but
  will require more mathematical effort to compute in the case of non-linear 
  estimation.
  
  <p>
    $x$ is a single scalar here. It can be a vector comprising multiple unknowns. 
  </p>
  We can convert our squared error criterion to vector notation as follows. 

  $$\begin{align*}
    \mathcal{L}_{LS}(x) = e^2_1 + e^2_2 + e^2_3 + e^2_4 &= \mathbf{e}^T\mathbf{e} \\
    &= (\mathbf{y} - \mathbf{H}x)^T(\mathbf{y} - \mathbf{H}x) \\
    &= \mathbf{y}^T\mathbf{y} - x^T\mathbf{H}^T\mathbf{y} - \mathbf{y}^T\mathbf{H}x + x^T\mathbf{H}^T\mathbf{H}x
  \end{align*}$$
  
  <button class="accordion">Selah</button>
  <div class="panel">
    $e^Te$, i.e. matrix transpose times the normal form is same as the sum of the squares of the elements of the matrix.

    $$\begin{align*}
      y &= H\hat{x} \\
      \hat{x} &= \frac{y}{H} \\
      &= \frac{H^Ty}{H^TH} = (H^TH)^{-1}H^Ty

    \end{align*}$$

    Why we set the derivative of $f(x)$ to 0 to solve for an unknown variable $x$: <a href="https://t.ly/WUKV">Ref1</a>. 
    
    <p>
      <m>When minimising the sum of squared errors criterion (imagining a plot with $e^2$ on y-axis vs $x$ on the x axis), how do we ensure we don't get stuck in a local maximum instead of the minimum when we set $dx=0$?</m>
    </p>
  </div>

  We need to minimize the squared error with respect to our true resistance $x$. 
  From calculus, we know that we can solve for an extremum (here, a minimum) 
  by taking the partial derivative of the function (here, squared error) w.r.t. 
  the unknown $x$ and setting the derivative to $0$. 
  
  $$\begin{align*}
    \frac{\partial L}{\partial x}\Bigr|_{x=\hat{x}} = 
    -y^TH - y^TH + 2\hat{x}^TH^TH = 0 \\
    -2y^TH + 2\hat{x}^TH^TH = 0
  \end{align*}$$

  Re-arranging, we arrive at what are called the <code>normal equations</code>, which can be 
  written as a single matrix formula. We can solve these to find $\hat{x}_{LS}$, 
  the resistance which minimizes our squared error criterion. 
    
  $$
    \hat{x}_{LS} = (H^TH)^{-1}H^Ty
  $$

  This expression has a unique solution, i.e. we will only be able to solve for $\hat{x}$ 
  if and only if $(H^TH)^{-1}$ is not singular i.e. if the matrix $(H^TH)$ has an inverse,
  i.e. if it is invertible. 
  
  If we have m measurements and n unknown parameters, then
  $$
    H \in \mathbb{R}^{m\times n} \quad\quad H^TH \in \mathbb{R}^{n\times n}
  $$
   
  The matrix exists (and therefore derive the least squares 
  solution) if and only if there are at least as many measurements as there are unknown 
  parameters. This will usually not be a problem. In fact, we'll often face the challenge of 
  dealing with too many measurements. 
  But nevertheless, keep this limitation in mind when working with the formula. 

  <p>
    For our example:
    $$
      y = \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996 \end{bmatrix}
      \quad
      H = \begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
    $$
    $\hat{x}_{LS} = (H^TH)^{-1}H^Ty$
    $$
      \begin{bmatrix}
      \begin{bmatrix}1 & 1 & 1 & 1 \end{bmatrix}
      \begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
      \end{bmatrix}^{-1}
      \begin{bmatrix}1 & 1 & 1 & 1 \end{bmatrix}
      \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996 \end{bmatrix}
      = \frac{1}{4}(1068 + 988 + 1002  996) = 1013.5 \Omega
    $$

    Note that the expression simplifies to the arithmetic mean of the 4 measurements. 
    Now, we have another justification for using the arithmetic mean, 
    it minimizes the simple least squares criterion. 
  </p>
</section>

<section id="3"><h1>Method of LS assumptions</h1>
  We've assumed that:
  <ul>
    <li>Our measurement model is linear</li>
    This is a very important assumption that is often broken in complex systems. 
    
    <p>
      <li>Our measurements have an equal weight in our error equation</li>
      We've assumed that we care about each of our measurements equally
    </p>
  </ul>
</section>

<section id="4"><h1>Weighted LS</h1>
  What if some of our measurements are of better quality than others i.e. there's
  varying measurement noise variance? For example, 1 sensor is better than the other.

  <p>
    From now on, 
    let's drop the assumption that we're only estimating one parameter and derive the 
    more general normal equations. This will allow us to formulate a way to estimate multiple 
    parameters at one time. For example, if we wanted to estimate several resistance values 
    at once. 
  </p>

  Let's begin by using the following general notation. We have $m$ 
  measurements that are related to $n$ unknown parameters through a linear model 
  represented by $H$, the Jacobian matrix whose form and entries will depend on the particular 
  problem at hand. 

  $$
    \begin{bmatrix}y_1 \\ . \\ . \\ y_m\end{bmatrix}
    = H\begin{bmatrix}x_1 \\ . \\ . \\ x_n\end{bmatrix}
    + \begin{bmatrix}v_1 \\ . \\ . \\ v_m\end{bmatrix}
  $$
  $$
    y = Hx + v
  $$
  
  One way to interpret the ordinary method of least squares is to say 
  that we are implicitly assuming that each noise term v_i is an independent random 
  variable across measurements and has an equal variance i.e. IID.

  $$
    \mathbb{E}[v^2_i] = \sigma^2\enspace (i=1,..,m)
    \qquad \qquad R = \mathbb{E}[vv^T]=
    \begin{bmatrix}
    \sigma^2 & & 0 \\
    & \ddots & \\
    0 & & \sigma^2    
    \end{bmatrix}
  $$
  
  $\mathbb{E}$ is the expected value. The expected value of the square of the deviation
  from the mean (here, the mean is assumed to be the true value) is the same as the 
  variance.
  
  <p>
    If we instead assume that each noise term is independent but has a
    different variance, we can define our noise covariance as follows.
  </p>

  $$
    \mathbb{E}[v^2_i] = \sigma^2_i \enspace (i=1,..,m)
    \qquad \qquad R = \mathbb{E}[vv^T]=
    \begin{bmatrix}
    \sigma_1^2 & & 0 \\
    & \ddots & \\
    0 & & \sigma_m^2    
    \end{bmatrix}
  $$

  We can define a weighted least squares criterion as:
  $$\begin{align*}
    \mathcal{L}_{WLS}(x) &= e^TR^{-1}e\\
    &= \frac{e^2_1}{\sigma^2_1} + \cdots + \frac{e^2_m}{\sigma^2_m}
  \end{align*}$$
  
  Each squared error term is now weighted by the inverse of the variance 
  associated with the corresponding measurement. In other words, the lower the 
  variance of the noise, the more strongly it's associated error term will 
  be weighted in the loss function. 
  
  <p>
    In the case of equal variances, the same parameters that 
    minimize our weighted least squares criterion will also minimize our ordinary least 
    squares criterion as we should expect. We do get $\frac{1}{\sigma^2}$; but since the 
    variance is constant, it will not affect the final estimate. 
  </p>

</section>

<section id="5"><h1>Minimising the weighted LS criterion</h1>
  Expanding the new criterion:

  $$\begin{align*}
    \mathcal{L}_{WLS}(x) &= e^TR^{-1}e\\
    &= (y-Hx)^TR^{-1}(y-Hx)
  \end{align*}$$

  We minimise the same way as before by taking a derivative. 
  In the general case where we have $n$ unknown parameters in our bold vector $x$, 
  this derivative will actually be a gradient. Setting the gradient to the 0 vector, 
  we then solve for our best or optimal parameter vector $\mathbf{\hat{x}}$. 
  $$
    \hat{x} = argmin_x\mathcal{L}(x) \qquad \longrightarrow \qquad
    \frac{\partial{\mathcal L}}{\partial x}\Bigr|_{x=\hat{x}}
    = 0 = -y^TR^{-1}H + \hat{x}^TH^TR^{-1}H
  $$
  This leads to another set of normal equations called the weighted normal equations. 
  $$\begin{align*}
    H^TR^{-1}H\hat{x}_{WLS} &= H^TR^{-1}y \\
    \hat{x} &= (H^TR^{-1}H)^{-1} H^TR^{-1}y
  \end{align*}$$

  An individual variance is assigned to each measurement, which means that the 
  matrix $\mathbf{R}$ (and its inverse) is $m \times m$ in size.
</section>
  
<section id="6"><h1>Weighted LS example</h1>
  <figure>
    <img style="height:70px; width:auto"
    src="../../../figures/drone/19_wlsExample.png"/>
  </figure>

  Noise is described in standard deviations, hence why they have the units of ohms.
  They need to be squared to get the variances.

  $$\begin{align*}
    \hat{x} &= (H^TR^{-1}H)^{-1} H^TR^{-1}y \\
    &=
    \begin{bmatrix}
      \begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix} &
      \begin{bmatrix}400&&& \\ &400&& \\ &&4& \\ &&&4 \end{bmatrix}^{-1} &
      \begin{bmatrix}1 \\ 1 \\ 1 \\ 1\end{bmatrix}
    \end{bmatrix}^{-1} 

    \begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix}
    \begin{bmatrix}400&&& \\ &400&& \\ &&4& \\ &&&4 \end{bmatrix}^{-1}
    \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996\end{bmatrix} \\

    &= \frac{1}{1/400 + 1/400 + 1/4 + 1/4}
    (\frac{1068}{400} + \frac{988}{400} + \frac{1002}{4} + \frac{996}{4}) \\
    &= 999.3\Omega
  \end{align*}$$

  Evaluating the weighted least squares solution, we can see that the 
  final resistance value is much closer to what the more accurate multimeter 
  measured.
</section>

<section id="7"><h1>Summary of WLS vs LS</h1>
  By using weighted least squares, we can vary the importance of each measurement to the 
  final estimate. It's important to be comfortable working with different measurement 
  variances and also with measurements that are sometimes correlated. 

  <p>
    <table class="table2 center">
      <tr>
        <th></th>
        <th>LSs</th>
        <th>WLSs</th>
      </tr>
      <tr>
        <td>Loss/Criterion</td>
        <td>$\mathcal L_{LS}(x) = e^Te$</td>
        <td>$\mathcal L_{WLS}(x) = e^TR^{-1}e$</td>
      </tr>
      <tr>
        <td>Solution</td>
        <td>$\hat{x}_{LS} = (H^TH)^{-1}H^Ty$</td>
        <td>$\hat{x}_{WLS} = (H^TR^{-1}H)^{-1}H^TR^{-1}y$</td>
      </tr>
      <tr>
        <td>Limitations</td>
        <td>$m\geq n$</td>
        <td>
          $m \geq n$ <br>
          $\sigma^2_i > 0$
        </td>
      </tr>
    </table>
  </p>

  We need to make sure that we model our error sources correctly.
  We derived the weighted 
  least squares criterion and the associated weighted normal equations that can be solved 
  to yield the weighted least squares estimate of a set of 'constant parameters'. 
  We need to modify the method of least squares to work 
  recursively, that is to compute an optimal estimate based on a stream of measurements
  without having to acquire the entire set beforehand, when we look at state estimation, 
  or the problem of estimating quantities that change continuously over time.
</section>

<section id="8"><h1>Independent and identically distributed</h1>
  What does it mean for a set of random variables to be independent and identically distributed?
  <ul>
    <li>
      Each random variable follows the same probability distribution and the variance of 
      any random variable does not depend on the other variables.
    </li>
    Close! It is true that, if the random variables all follow the same probability 
    distribution, each must have the same variance (which is fixed). 
    However, the term "i.i.d" refers to the statistical independence of any pair of 
    random variables.
    <p>
      <li>
        Each random variable follows the same probability distribution and all the variables 
        are mutually independent (i.e., the cross-covariance of any pair is zero).
      </li>
      Right! This relationship is shown on the last slide, where the entries on the main 
      diagonal of the matrix $R$ are the same and the off-diagonal entries are all 
      equal to zero.
    </p>
  </ul>
  






</section>

<section id="9"><h1>LS examples</h1>
  <ul>
    <li>Example 1 </li>
    V=IR. We don't know R. We collect lots of data related to V and I,
    and fit a straight line through the points. The slope gives R.
    Include reference to jlib documentation. Start jlib documentation from
    index.
    <a href="../">Test</a>
  </ul>
</section>

<section id="10"><h1>Musings</h1>
  <ul>
    <li>Ordinary LS measurement variance</li>
    If the 4 resistance measurements are taken by different sensors;
    what do we mean when we say the variances are different: 

    <p>
      For each of the 4 sensors, take 100 measurements and work out the 
      variance. Variance is the sum of deviation from the mean times the probability
      of occurrence of the corresponding measurement. 
      I'm guessing a manufacturer will need to provide this info when they do tests of 
      measuring known physical properties. In most cases, they'd probably assume
      the probability of each of the 100 measurements is the same. So, the variance
      just becomes the mean of the squared deviations.
    </p>

    <p>
      <li>
        Just to confirm, when someone says the variance is $x$, it gives no indication
        of the accuracy right? It only gives an indication of the bias of each sensor
        from whatever the mean is. I think <a href="https://bit.ly/3VB6OnG">this post</a>
        is confirming my thoughts are correct. If the sensor is accurate, the mean
        is the actual value of the physical quantity.
      </li>
    </p>

    <li>Is sensor variance a measure of accuracy</li>
    Googling this question gave led to many interesting link in reasearch gate and
    different stack exchanges.

    <p>
      <li>
        If we're doing batch LS on $y=Hx+b$ instead of just $y=Hx$,
        does the formula change?
      </li>
      I think it's equivalent to $y=H_1x+H_2b$
    </p>

    <li>Understand the derivation of RLS</li>
    <a href="https://t.ly/tzde">This</a> might be a good derivation.
    <a href="https://t.ly/c-GU">This</a> could be interesting as well.
  </ul>
</section>

<section id="11"><h1>Another intro</h1>
  The standard least squares approach applied to estimating the state of a flying car, would make the following unrealistic assumptions:

  <ul>
    <li>The vehicle is stationary</li>
    <li>The measurements come in all at once</li>
    <li>The underlying measurement model $H$ is noisy but linear</li>
    Measurements of things like orientation are rarely linear.
  </ul>

  A linear function of $x$, which in this case maybe a vehicle position, can be written as:
  $$
    y = Ax + b
  $$
  
  Despite these over-simplifications/assumptions, this estimation technique is common for calibration, finding the transform between two coordinate systems or model fitting.

  <subsection id="11.0"><h1>Example</h1>
    Let's consider a system where we make $m$ measurements $y$ of a constant state $x$, such that:
    $$
      \tilde{y} = Hx + v
    $$
    
    $x$ is the constant unknown state vector of length $n$ that we're trying to estimate.
    <br>
    $\tilde{y}$ is the specific vector of measurements of length $m$. tilde denotes the actual measurements received. The measurements are a linear function of the state perturbed by some unknown noise.
    <br>
    $v$ is the unknown error vector. The noise is assumed to be drawn from a Gaussian distribution with a $0$ mean and some covariance $R$, which perhaps can be assumed to be some known variance times the identity matrix.
    <br>
    $H$ is the measurement matrix. It's assumed to be constant and known and it projects the state into the measurement space.

    <p>
      If we knew the error $v$, we could solve for $x$: $Hx = \tilde{y} - v$. Or if we knew $x$, we can solve for $v$. We don't know either of these and that's what makes this an estimation problem. We're going to use a maximum likelihood estimation approach. Intuitively, what we're going to do is look at our measurements and ask what vehicle state is the most likely to yield this particular set of measurements.
    </p>
  </subsection>
  
</section>

<section id="12"><h1>Deriving The Maximum Likelihood Estimator</h1>
  We want to know what vehicle state is most consistent with our measurements. Mathematically, the goal is to select our estimate $\hat{x}$ to be the value of $x$ that maximizes the probability of our actual measurements $\tilde{y}$:
  <br>
  Goal: Choose $\hat{x}$ to maximise $p(y=\tilde{y} | x = \hat{x})$

  <p>
    There are two steps to this process:
    <ul>
      <li>Find $p(y|x)$</li>
      The probability density function $p(y|x)$ in this model, tells us the likelihood of the measurement y when the state is x .

      <p>
        <li>Choose $\hat{x}$</li>
        Select the estimate $\hat{x}$ to be the value that yields a maximum likelihood of the actual measurements $\tilde{y}$ given the estimate $\hat{x}$.
      </p>
    </ul>
  </p>

  <subsection id="12.0"><h1>Breaking down the derivation steps</h1>

  </subsection>
    We're going to assume that the distribution $p(y|x)$ is Gaussian. We need an expression for the parameters of the distribution:

    <ul>
      <li>Mean</li>
      Since $y = Hx + v$ and $v$ is a zero-mean gaussian, it's easy to show that the expected value, the mean, is: $\bar{y} = Hx$

      <p>
        <li>Covariance</li>
        Given the properties of our noise model, $v \sim \mathcal{N}(0,R)$, it's also easy to show that the covariance $(y-\bar{y})(y-\bar{y})^T$ is going to be equal to the covariance of just the noise, $R$.
      </p>
    </ul>

    Thus, $p(y|x) = \mathcal{N}(Hx, R)$

    <p>
      We just need to find a value of x-hat that maximizes the likelihood of the actual measurements y-tilde given x-hat.
    </p>
    
    Writing out the formula for the above Gaussian:
    $$
      p(\tilde{y}|x) = \underbrace{
          \frac{1}{(2\pi)^{\frac{n}{2}}\lvert{R}\rvert^{\frac{1}{2}}}
        }_{\text{Normalizer}}
      e^
        \underbrace{
          {-\frac{1}{2}(\tilde{y}-Hx)^TR^{-1}(\tilde{y}-Hx)}
        }_{\text{Term to maximise}}
    $$

    The normalizer term does not depend on $x$ at all. For the likelihood of the received measurements of y-tilde given x, the specific choice of x has no impact on the normalizer and can be ignored.

    <p>
      To maximise this overall function, we need to maximise the term that's in the exponent; which means we just need to maximize the function in the exponent.
    </p>


    A standard way to solve for the maximum of a function is to take the derivative and find where it is zero. If we take the derivative of the exponent, set it equal to zero, and then solve for $x$, we get:
    $$
      \hat{x} = (H^TH)^{-1}H^T\tilde{y}
    $$

    This is a solution to the least squares estimation problem given some measurements $\tilde{y}$ that are linearly related to some unknown quantity $x$ by the matrix $H$ that we assume we know. Using this, we can solve for the most likely estimate, $\hat{x}$.
</section>

<section id="13"><h1>Fitting A Line With Linear Least Squares</h1>
  [<a href="https://t.ly/mwtO">Good quick tutorial</a>].
  <br>
  We now have the equation for the maximum likelihood estimator $\hat{x}$. The best estimate is purely a function of the measurement model $H$ and the actual measurements $\tilde{y}$.

  <subsection id="13.0"><h1>Example</h1>
    We're going to fit some noisy data to a straight line, given by the equation:
    $$
      y = at + b
    $$

    <div class="container">
      <figure>
        <img style="height:125px; width:auto"
        src="../../../figures/drone/26_leastSquareLineFit.png"/>
        <figcaption>
          Straight line representing noisy data
        </figcaption>
      </figure>
    </div>
  </subsection>

Let's assume that our data is sampled from the line with some added noise. What we actually want are the properties of the line, which form our unknown state vector $x$:
$$
  x = \begin{bmatrix}
  a \\ b
  \end{bmatrix}
$$

The model assumes that our measurements $y_1, y_2, \cdots$ are drawn according to a linear function $y_i$:
$$
  y_i = at_i + b + v_i
$$
It's the unknown $v_i$ that's perturbing the measurements from the line. The measurements are in the form of:
$$
  \tilde{y} = Hx + v
$$

We have the state vector $x$ and all the measured values: 
$$\begin{bmatrix}
  \tilde{y_1} \\
  \vdots \\
  \tilde{y_m}
\end{bmatrix}$$

In order to get the vector $y_i$ to match up with the vector $\tilde{y}$, we need an $H$ that looks as follows:
$$\begin{bmatrix}
  t_1 & 1 \\
  \vdots & \vdots \\
  t_m & 1
\end{bmatrix}$$

Now that we have our measurements, we have our $H$, we can use the equation for the maximum likelihood estimate to find the best estimate for x, $\hat{x}$. 

<p>
  An important thing to note is that once we have the measurements $\tilde{y}$, the least squares estimation problem boils down to figuring out exactly what the measurement model $H$ looks like. Figuring out how to form $H$ for any estimation problem is often the tricky bit.
</p>
</section>

<section id="14"><h1>The problem with non-linearities</h1>
  We want to infer the location of a vehicle from position measurements which are received one at a time when the vehicle is stationary. But now the measurements are noisy and a nonlinear function of the position. Maybe the measurements tell us something about the orientation.

  $$
    \tilde{y} = h(x) + v
  $$
  $x$ is a constant unknown state vector of length $n$.
  <br>
  $\tilde{y}$ is the vector of $m$ measurements (of $x$) that is actually received.
  <br>
  $v$ is the unknown error/noise vector with zero mean and known covariance
  <br>
  $h$ is the measurement function (<n>constant and known</n>, but non-linear)

  <p>
    Before we had a linear function which meant we could write the function as a matrix $H$ times $x$.
  </p>

  $h$ is now a problem. In recursive estimation, we were projecting the measurement distribution into the same state space as the estimate. We had our estimate $x$ and $p(x)$. We had some distribution of measurements $y$ and $p(y)$. The mean of the distribution was given by $Hx$ and the covariance was $R$. 
  
  <div class="container">
    <figure>
      <img style="height:180px; width:auto"
      src="../../../figures/drone/27_relationBetweenMeasAndStateSpace.png"/>
      <figcaption>
        Linear relationship between the measurement and the state/parameter space
      </figcaption>
    </figure>
  </div>
  We could use the linear matrix $H$ to project Gaussian distributions in the measurement space into the state space. The fact that the measurements were related by a linear function ($H$) ensured that the projected distribution stayed Gaussian. That meant that the weighted average posterior distribution was Gaussian as well.

  <div class="container">
    <figure>
      <img style="height:180px; width:auto"
      src="../../../figures/drone/28_nonLinearRelationBetweenMeasAndStateSpace.png"/>
      <figcaption>
        Non-Linear relationship between the measurement and the state/parameter space
      </figcaption>
    </figure>
  </div>
  But if $h$ is an arbitrary function, we have no guarantees that the projected distribution of $y$ will stay Gaussian; which makes it a lot harder to take a weighted average between the prior $\hat{x}_0$ and what the new measurements are telling us.

  <p>
    The problem is, if we want to preserve the Gaussian to Gaussian mapping, we need to find a way to make the measurement model linear.
  </p>
</section>

<section id="15"><h1>Calculating the Jacobian</h1>
  A common thing to do when linearising a nonlinear function, is to take the Taylor series approximation of $h$ around the estimate $\hat{x}_0$:

  $$
    h(x) \approx h(\hat{x}_0) + H_{\hat{x}_0}(x-\hat{x}_0)
  $$
  
  $H_{\hat{x}_0}$ is the first derivative of $h$ evaluated at $\hat{x}_0$.
  <br>
  Notice the 'approximately equal to' sign.

  <p>
    This is similar to some of the linearization done in controls. If $x$ and $y$ are scalars, then the first derivative is a scalar as well. $H_{\hat{x}_0}$ would be a simple derivative.
  </p>

  But if $x$ and $y$ are vectors of length $n$ and $m$ respectively, then from linear algebra, the first derivative is a matrix called the <i>jacobian</i>. $H_{\hat{x}_0}$ would be equal to a matrix with the individual partial derivative components in the various locations in the matrix.

  $$
    H_{\hat{x}_0} = \begin{bmatrix}
    \frac{\partial{h_1}}{\partial{x_1}} & \cdots & 
    \frac{\partial{h_m}}{\partial{x_1}} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial{h_1}}{\partial{x_n}} & \cdots &
    \frac{\partial{h_m}}{\partial{x_n}}
    \end{bmatrix}
  $$

  Since we know both $\hat{x}_0$ and $h$, we can compute the jacobian and the function evaluated at the prior mean $h(\hat{x}_0)$, which lets us rewrite all of this in a linear form which is the form we need for a cursive estimation to work:

  $$
    h(x) \approx Ax + b
  $$
  
  As a result the maximum a posteriori algorithm is basically unchanged, with just an extra step of linearization; that is constructing the jacobian.

  <p>
    Our three steps are now:
    <ul>
      <li>Construct the jacobian $H_{\hat{x}_0}$ to get a linear approximation of $h$ around the estimated state</li>

      <p>
        <li>Compute the posterior covariance</li>
        $$
          Q_1 = (Q^{-1}_0 + H^T_{\hat{x}_0}R^{-1}H_{\hat{x}_0})^{-1}
        $$
      </p>

      <li>Compute the posterior mean estimate</li>
      $$
        \hat{x}_1 = \hat{x}_0 + Q_1H^T_{\hat{x}_0}R^{-1}(\tilde{y} - h(\hat{x}_0))
      $$
    </ul>
  </p>
  
  We now have the ability to take in a series of measurements of an unknown quantity, where the measurements are non-linear function of the unknown quantity. Assuming that we have gaussian noise being injected into the system to perturb the measurements, we can still recover a reasonable estimate.

  <p>
    We'll see how specific sensors can plug into this formulation, to relax the assumption about the vehicle moving using the Kalman filter.
  </p>
</section>
</chapter>

</body>
</html>
