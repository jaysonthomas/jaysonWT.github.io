
<!DOCTYPE html>
<html>
<head>
  <title>Batch Linear Least Squares</title>
  <meta name="Batch Linear Least Squares" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../../logbook.js"></script>

  <script src="../../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../../pages/bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Measurement model</a>
<a href="#1">Noise model</a>
<a href="#2">Cost function</a>
<ul class="no-bullets">
  <li><a href="#2.0">Cost function in matrix notation</a></li>
</ul>
<a href="#3">Minimising the cost function to find the estimate</a>
<a href="#4">Limitation</a>
<a href="#5">Example</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Batch Linear Least Squares</h1>

<section id="0"><h1>Measurement model</h1>
  $$\text{Measurement model}: y = x+v$$
  $x$ is the constant parameter value to be estimated for example.
</section>

<section id="1"><h1>Noise model</h1>
  Assume 4 measurements are collected:

  $$\begin{align*}
    y_1 &= x + v_1 \\
    y_2 &= x + v_2 \\
    y_3 &= x + v_3 \\
    y_4 &= x + v_4 \\
  \end{align*}$$
  
  Treating the noise as equivalent to a general error, i.e. <n>not interpreting the noise from a probabilistic perspective</n>, for each of the four measurements, $v_i$ would be a scalar noise term that is independent of the other noise terms. In other words, the noise is considered to be <code>independent and identically distributed</code> or <code>IID</code>. 
</section>  

<section id="2"><h1>Cost function</h1>
  The error between each measurement and the unknown value to be estimated can be defined as:
  $$
    e_i = y_i - H_ix
  $$

  The method of least squares states that the best estimate of $x$ ($\hat{x}_{LS}$ or $\mathcal{L}_{LS}(x)$) is one which minimizes the <i>squared error criterion</i>, also called the <i>squared error cost function</i> or <code>loss function</code>; i.e. the sum of squared errors.

  $$
    \hat{x}_{LS} = argmin_x(e^2_1 + e^2_2 + e^2_3 + e^2_4)
  $$

  <subsection id="2.0"><h1>Cost function in matrix notation</h1>
    Expressing the errors in matrix notation is helpful when having to deal with thousands of measurements.

    $$\begin{gather*}
      \begin{bmatrix}e_1 \\ e_2 \\ e_3 \\ e_4\end{bmatrix}
      = \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4\end{bmatrix} -
      \begin{bmatrix}1 \\ 1 \\ 1 \\ 1\end{bmatrix}x
    \end{gather*}$$
    
    $H$, the <i>Jacobian matrix</i>, has the dimensions $m\times n$, where $m$ is the number of measurements and $n$ is the number of unknowns to be estimated. $H$ is easy to compute in the linear case <n>but requires more mathematical effort in the case of non-linear estimation</n>. 
    
    <p>
      $x$ can be a vector comprising multiple unknowns.
    </p>
    
    The squared error criterion can be expressed in vector notation as follows:
    
    $$\begin{align*}
      \mathcal{L}_{LS}(x) = e^2_1 + e^2_2 + e^2_3 + e^2_4 &= e^Te \\
      &= (y - Hx)^T(y - Hx) \\
      &= y^Ty - x^TH^Ty - y^THx + x^TH^THx
    \end{align*}$$
  </subsection>
</section>

<section id="3"><h1>Minimising the cost function to find the estimate</h1>
  <button class="accordion">Selah</button>
  <div class="panel">
    $e^Te$, i.e. matrix transpose times the normal form is same as the sum of the squares of the elements of the matrix.

    <p>
      Why we set the derivative of $f(x)$ to 0 to solve for an unknown variable $x$: <a href="https://t.ly/WUKV">Ref1</a>.
    </p>
    
    <m>When minimising the sum of squared errors criterion (imagining a plot with $e^2$ on y-axis vs $x$ on the x axis), how do we ensure we don't get stuck in a local maximum instead of the minimum when we set the slope to $0$?</m>
  </div>

  The aim is to minimise the squared error with respect to the unknown $x$. From calculus, an extremum (here, a minimum) can be solved for by setting the partial derivative of the function (here, squared error) w.r.t. 
  the unknown $x$ to $0$. 
  
  $$\begin{align*}
    \frac{\partial L}{\partial x}\Bigr|_{x=\hat{x}} = 
    -y^TH - y^TH + 2\hat{x}^TH^TH = 0 \\
    -2y^TH + 2\hat{x}^TH^TH = 0
  \end{align*}$$

  Re-arranging: 
    
  $$
    \hat{x}_{LS} = (H^TH)^{-1}H^Ty
  $$

  This is called the <i>normal equations</i>. This is solved to find $\hat{x}_{LS}$, the estimate which minimizes the squared error criterion. 
</section>

<section id="4"><h1>Limitation</h1>
  The batch LS equation has a <i>unique solution</i>, i.e. $\hat{x}$ can only be solved for if and only if $(H^TH)^{-1}$ is <n>not singular</n> i.e. if the matrix $(H^TH)$ has an <i>inverse</i>, i.e. if it is <i>invertible</i>. 
  
  If we have m measurements and n unknown parameters, then
  $$
    H \in \mathbb{R}^{m\times n} \quad\quad H^TH \in \mathbb{R}^{n\times n}
  $$
  
  The Jacobian matrix only exists if there are at least as many measurements as there are unknowns, which is usually not a problem. Often, there are too many measurements. 
</section>

<section id="5"><h1>Example</h1>
  Consider a parameter estimation example: The aim is to measure the value in ohms of a simple resistor within the drive system of an autonomous vehicle. 4 separate measurements are collected sequentially using a multimeter. Assume the true resistor value is $1k\Omega$. But, this value could be wrong or the multimeter could be inaccurate or there could be some human error in taking the measurements. 

  <p>
    Let $x$ be the actual resistance. Assume it is a constant, but unknown.
    Measurements, $y$, of the resistance are collected:
  </p>
  
  $$
    y = \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996 \end{bmatrix}
    \quad
    H = \begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
  $$
  
  $$
    \hat{x}_{LS} = (H^TH)^{-1}H^Ty
  $$
  
  $$
    \begin{bmatrix}
    \begin{bmatrix}1 & 1 & 1 & 1 \end{bmatrix}
    \begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
    \end{bmatrix}^{-1}
    \begin{bmatrix}1 & 1 & 1 & 1 \end{bmatrix}
    \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996 \end{bmatrix}
    = \frac{1}{4}(1068 + 988 + 1002 + 996) = 1013.5 \Omega
  $$

  Note that the expression simplifies to the arithmetic mean of the 4 measurements. In other words, minimising the simple least squares criterion leads to computing the arithmetic mean in the above example. 
</section>
</chapter>

</body>
</html>
