
<!DOCTYPE html>
<html>
<head>
  <title>Continuous Probability</title>
  <meta name="Continuous Probability" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../pages/bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Continuous samples space</a>
<a href="#1">Cumulative distribution function</a>
<a href="#2">Probability density function</a>
<a href="#3">Parameters of a distribution</a>
<a href="#4">Common probability distributions</a>
<ul class="no-bullets">
  <li><a href="#4.0">Uniform distribution</a></li>
  <li><a href="#4.1">Gaussian or normal distribution</a></li>
</ul>
<a href="#5">What does a distribution have to do with robotics?</a>
<a href="#6">Estimating parameters from data</a>
<a href="#7">Drawing samples from a uniform distribution</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Continuous Probability</h1>

<section id="0"><h1>Continuous samples space</h1>
  An example of a continuous samples space is the real value position of a robot: <a href="https://t.ly/_1lN">Slide 11</a>.
  Random variables become extremely powerful in this case.

  <p>
    When dealing with a continuous sample space, we can no longer say anything about the probability that a vehicle is at some specific location, because for a continuous sample space, the probability of any particular value will always be 0. We can only give the probability that the vehicle is in some region.
  </p>
</section>

<section id="1"><h1>Cumulative distribution function</h1>
  [<a href="https://t.ly/LlT8">Further reading</a>]
  <figure>
    <img style="height:85px; width:auto"
    src="../../../figures/drone/8_cdf.png"/>
    <figcaption>
      CDF
    </figcaption>
  </figure>
  
  $$
    F_x(u) = p(x \leq u)
  $$

  The function describes the probability that a random variable $x$ is less than or equal
  to $u$. 
  
  <p>
    A <code>limit</code> is a value toward which an expression converges as one or more 
    variables approach certain values.
  </p>

  <p>
    $F$ will be monotonically increasing.
    The limit of $F$ as $u$ goes to $-\infty$ is $0$,
    and the limit as $u$ goes to $+\infty$ is $1$.
    It's important to understand that the index to the function is $u$,
    but it is sub-scripted by $x$ to indicate which random variable we're characterizing by
    the probability that the value of that random variable is less than $u$ or not.
  </p>
</section>
  
<section id="2"><h1>Probability density function</h1>
  [<a href="https://t.ly/L1RY">Great ref</a>]
  <br>
  [<a href="https://t.ly/WqWw">Ref</a>] I like the different probability distributions described here. Not sure the definition for PDF is accurate.
  <p>
    <a href="https://t.ly/1iwF">Ref</a> - why is it referred to as density?!
    <br>
    Refer to <code>uniformDist</code> in <a href="../../../jlib/docs/leastSquaresEstimation.html">Jlib docs</a>.
  </p>
  $$
    f_x(u) = \frac{d}{du}F_x(u)
  $$
  It's the derivative of a CDF w.r.t. its index.

  <figure>
    <img style="height:100px; width:auto"
    src="../../../figures/drone/9_pdf.png"/>
    <figcaption>
      Uniform PDF for the position $x$ of a vehicle
    </figcaption>
  </figure>

  <p>
    Consider this uniform PDF for the position $x$ of a vehicle.
    It has the value 0.5 between $x$ equals zero and two, and zero everywhere else.
    The probability that the vehicle is in the region between 0 and 2 is found
    by calculating the area under the curve which would be $2*0.5$.
    In fact, the probability that a random variable will take a value between 
    $x_1$ and $x_2$, is given by the integral of the probability 
    density function from $x_1$ to $x_2$.

    $$
      p(x_1 < x < x_2) = \int^{x_2}_{x_1}f_x(x)dx = \int^{x_2}_{x_1}p(x)dx
    $$
  </p>
  <p>
    $f_x(x)$ isn't the probability, and it can easily be greater than $1$.
    But we do often write this density function as $p(x)$.
    This changes our notion of what it means to be a proper distribution.
    To be a proper density, $f_x(u)$ has to be greater than or equal to $0$,
    and the integral under the distribution has to be equal to $1$.
    <i>Normalization</i> implies finding a constant multiplier in
    front of this density function to make it integrate to $1$.
  </p>
</section>

<section id="3"><h1>Parameters of a distribution</h1>
  We will use $\theta$ to collectively refer to all of the parameters that describe a particular distribution. For example, a continuous distribution like a Gaussian, can be fully described by two parameters $\mu$ and $\sigma^2$ which when taken together form part of $\theta$ for the distribution.

  <p>
    $p$ of $x$ given $\theta$ is not the same as the function that describes the density of $x$, parameterized by $\theta$.

    $$p(x|\theta) \neq p(x;\theta)$$
    $p(x|\theta)$ implies $\theta$ is a random variable itself.
  </p>
</section>

<section id="4"><h1>Common probability distributions</h1>
  [<a href="https://t.ly/5ik0s">Ref</a>] A probability distribution refers to a probability density function in the continuous space and to a probability mass function in the discrete space. CDF is common to both.
  There are two common distributions.

  <subsection id="4.0"><h1>Uniform distribution</h1>
    A uniform distribution can be parameterised in 2 ways:
    <ul>
      <li>Using $a$ and $b$ bounds</li>
      <figure>
        <img style="height:100px; width:auto"
        src="../../../figures/drone/10_udf.png"/>
      </figure>

      <li>Using the mean $\mu$ and the width $w$</li>
      <figure>
        <img style="height:100px; width:auto"
        src="../../../figures/drone/11_uniformDF_parameterisation.png"/>
      </figure>
      We could say the ends are $\mu - w$ and $\mu + w$,
      and the height of the distribution is $\frac{1}{2w}$.

      This distribution says all possible outcomes are equally likely,
      and sometimes this is the only thing we can do when we have
      no idea what the parametric family $p(x)$ actually is.
    </ul>
  </subsection>

  <subsection id="4.1"><h1>Gaussian or normal distribution</h1>
    <div class="container">
      <figure>
        <img style="height:80px; width:auto"
        src="../../../figures/drone/12_wideGaussian.png"/>
        <figcaption>
          Wide gaussian
        </figcaption>
      </figure>
  
      <figure>
        <img style="height:80px; width:auto"
        src="../../../figures/drone/13_thinGaussian.png"/>
        <figcaption>
          Thin gaussian
        </figcaption>
      </figure>
    </div>

    The gaussian is the familiar bell shaped curve, centered at some mean $\mu$,
    with some width given by the standard deviation $\sigma$. 
    The density function is given by:
    $$
      p(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}
    $$
        
    The Gaussian has a few interesting properties.
    <ul>
      <li>It's symmetric around the mean</li>
      <li>
        The probability that $x$ is within $\pm 1\sigma$ is <code>68%</code>
        and the probability that it's within $\pm 3\sigma$ is <code>99.7%</code>
      </li>
      <p>
        Notice how $\sigma$ describes how wide the distribution is.
        A small $\sigma$ will make the peak narrow and sharp, 
        implying that we have a high confidence of predicting what a sample drawn from 
        $x$ will be. A large $\sigma$ will make the normal distribution very wide and flat,
        implying that we have poor knowledge of what exactly 
        any given sample from this distribution will be.
      </p>
    </ul>
  </subsection>
</section>

<section id="5"><h1>What does a distribution have to do with robotics?</h1>
  Solving estimation problems often boils down to using data from sensors which are imperfect.

  <p>
    For example, a range finder pointed at a wall 1m away will most likely give measurements that are slightly different than 1 meter like 0.992, 1.001... etc. But if we repeat this measurement many times, we'll likely find that on average the sensor does measure the correct distance plus or minus some error. This error is called <i>sensor noise</i>,
    and it's often best modeled as a Gaussian random variable with some mean ideally $0$, and some standard deviation.
  </p>

  Therefore, in order to use a sensor more effectively, we need an accurate estimate of what the parameters of its output distribution are, i.e. we need a <i>sensor model</i>. <n>We can't know the exact values of the parameters of a distribution given some data</n>, but we can compute estimates of the parameters in a variety of ways.
  
  <p>
    A good sensor will have a small variance. A bad sensor will have a large variance. Before we put a sensor on a robot we need to estimate what this variance is.
  </p>
</section>

<section id="6"><h1>Estimating parameters from data</h1>
  <p>
    For example, a Gaussian is parameterized by $\mu$ and $\sigma^2$. We'll write the estimates of these parameters as $\hat{x}$, and $\hat\sigma^2$ respectively. Now, let's imagine that we have some Gaussian that's generating samples from $x$, with probability density $p(x)$, and we don't know $\mu$, or $\sigma$.
  </p>

  <figure>
    <img style="height:150px; width:auto"
    src="../../../figures/drone/14_estimatingParametersExample.png"/>
  </figure>
  
  If we repeatedly draw from this distribution to collect samples of data,
  we'll likely get more data near the mean, and fewer samples out on the tails of the gaussian. Using a technique called the <i>method of moments</i>, we can show that if we have samples $\{x_1, x_2,.., x_n\}$, then 
  $$\hat{x} = \frac{1}{n} \sum^n_{i=1}x_i$$
  $$\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i-\hat{x})^2$$

  <p>
    It's quite convenient that the mean of the distribution matches the expected value,
    and the variance of the distribution matches that variance.
  </p>
</section>

<section id="7"><h1>Drawing samples from a uniform distribution</h1>
  Why it's okay for the probability density function to be greater than 1 [<a href="https://t.ly/2zE0">Ref</a>].

  <p>
    Drawing samples from a uniform distribution means randomly selecting values from a continuous range of values with an equal probability of selecting any value within that range. The probability density function of a uniform distribution is a constant, and the probability of selecting any given value within the range is the same. 
  </p>
</section>

</chapter>

</body>
</html>
