
<!DOCTYPE html>
<html>
<head>
  <title>Discrete Probability</title>
  <meta name="Discrete Probability" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../pages/bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Example</a>
<a href="#1">Random variable</a>
<a href="#2">Expected value</a>
<a href="#3">Variance</a>
<ul class="no-bullets">
  <li><a href="#3.0">Equation deepdive</a></li>
  <li><a href="#3.1">Sample variance</a></li>
  <li><a href="#3.2">Covariance</a></li>
</ul>
<a href="#4">Relationship of covariance and expectation</a>
<a href="#5">Z-score</a>
<a href="#6">Normalisation</a>
<a href="#7">Probability mass function</a>
<a href="#8">Definitions related to probability</a>
<a href="#9">Musings</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Discrete Probability</h1>

<section id="0"><h1>Example</h1>
  We are running tests to see if a bot comes to a stop at exactly the right 
  point. We run the test a 1000 times. At the end of each test we
  measure its position w.r.t. the target. The results are as shown 
  <a href="https://t.ly/3nOj">here</a>.

  $$
    P(Bot\, stops\, 1mm\, from\, target) = 
    \frac{Frequency\, of\, particular\, event}
    {Total\, number\, of\, events} = \frac{900}{1000} = 0.9
  $$

  Random variable, $X$, represents the event, i.e. the error in the final position.
  <br>
  Example, $x_2 = 2000$, $P(X=x_2) = 0.05$.
</section>

<section id="1"><h1>Random variable</h1>
  A <code>random variable</code> has more than one possible values. When
  actually measured, the variable will take one of these values at random, each with a
  certain probability. For example, the side on which a coin lands is a random variable with
  two possible values: heads and tails, each with a probability of 0.5.
  It takes on integer values to represent an event if the event's name 
  is in string format

  <p>
    A <code>deterministic variable</code> is a variable with only one possible fixed 
    value at any given time. For example, a person's age is a deterministic variable.
  </p>
</section>

<section id="2"><h1>Expected value</h1>
  It is not the most likely, but the average value that we'd expect to
  get if we were to continue drawing from a distribution over and over again.
  It gives the <n>long-run average</n> value of a <n>repeated</n> probabilistic experiment.
  $$
    E(X) = \sum_{i=1}^{N}x_iP(X=x_i)
  $$
  It's the value of the random variable, times the likelihood of each of 
  those outcomes all summed together. When the probability of each event is
  $\frac{1}{N}$, the expected value is equal to the mean of the distribution.

  <p>
    The expected value of a random variable, where the underlying events are truly 
    non-numeric quantities such as working or not working, may not be useful because
    the values assigned to the random variables don't make much sense.
    But in the example above, where the underlying events (final position of the bot)
    are numeric events, the expected value tells us something about
    what we might expect to see at any given time. 
  </p>
</section>

<section id="3"><h1>Variance</h1>
  <p>
    I had asked <a href="https://t.ly/CUyG">this</a> question on stackexchange. Another good <a href="https://t.ly/-5LN">reference</a>.   <a href="https://t.ly/jWCR">Code example</a>.
  </p>
  
  When drawing from a distribution over and over again, the <i>variance</i> gives an indication of how spread out the numbers or <i>samples</i> (the values being drawn) will be from the mean(the expected value need not be
  equal to the mean). The variance is not a measure of how well we know the mean, but just how much the samples vary.

  $$
    \sigma^2 = \sum_{i=1}^{N}(x_i-\mu)^2P(X=x_i)
  $$

  $\sigma$ is the <i>standard deviation</i> which is the square root of variance. 

  <p>
    When the probability of every outcome $P(x_i)$ is $\frac{1}{N}$, the variance 
    is just the average of the squared differences from the mean.

    $$
      \sigma^2 = \sum_{i=1}^{N}\frac{(x_i-\mu)^2}{N}
    $$
  </p>
  
  <subsection id="3.0"><h1>Equation deepdive</h1>
    Subtracting the mean makes the mean the zero point. It is a shift
    operation. We can now represent everything w.r.t the mean as simple
    <i>deviations</i> as shown <a href="https://t.ly/dipKw">here</a>.
    Squaring the deviations as opposed to taking the raw or absolute values 
    (i.e. not squaring), helps to amplify the deviations of samples from the 
    mean which is helpful when comparing the variances of 2 different distributions.
  </subsection>  

  <subsection id="3.1"><h1>Sample variance</h1>
    Assuming the probability of every observation is the same, in calculating <i>sample variance</i>, we divide by $N-1$ instead of $N$:

    $$
    \sigma^2 = \sum_{i=1}^{N}\frac{(x_i-\mu)^2}{N-1}
    $$
    Dividing by $N - 1$ adjusts for the fact that the sample mean is itself an estimate based on the data, and is not known with certainty. This adjustment reduces the bias in the estimate of the population variance, making it an <i>unbiased estimator</i> of the population variance.

    <p>
      When we divide by $N$, the resulting estimate is biased low, meaning it tends to underestimate the population variance. This bias can be substantial for small samples, but becomes negligible as the sample size increases.
    </p>
  </subsection>

  <subsection id="3.2"><h1>Covariance</h1>
    The <i>covariance matrix</i> is a square matrix that summarizes the pairwise correlations between the variables in a dataset. It is <i>symmetric</i> when the covariance between variables $i$ and $j$ (2D example) is the same as that between variables $j$ and $i$.
  </subsection>
</section>

<section id="4"><h1>Relationship of covariance and expectation</h1>
  $$\begin{align*}
  cov(x_1, x_2) &= E[(x_1 - \mu_{x_1})(x_2 - \mu_{x_2})] \\
                &= E[(x_1 - E[x_1])(x_2 - E[x_2])] \\ 
                &= E[x_1x_2 - x_1E[x_2] - x_2E[x_1] + E[x_1]E[x_2]] \\ 
  \end{align*}$$

  Using <i>linearity of the expectation</i>, where if $a,b$ are constants and $X,Y$ are random variables, then:
  $$
    E[aX + bY] = aE[X] + bE[Y]
  $$
  In other words, the expected value of a linear combination of random variables is equal to the linear combination of their expected values, with the same coefficients.

  <p>
    The covariance becomes:
    $$\begin{align*}
    cov(x_1, x_2) &= E[x_1x_2] - E[x_1]E[x_2] - E[x_2]E[x_1] + E[x_1]E[x_2] \\ 
                  &= E[x_1x_2] - E[x_1]E[x_2]
    \end{align*}$$
  
  </p>

</section>

<section id="5"><h1>Z-score</h1>
  It is the number of standard deviations from the mean.
  Steps in calculating:
  <ul>
    <li>
      For each number: subtract the mean, i.e. centre the distribution
      at the mean. This difference will give us each number's deviation
      from the mean.
    </li>
    <li>
      Given a deviation, to find out how many standard deviations that is,
      divide it by a single standard deviation.
    </li>
    $$
      zScore = \frac{x-\mu}{\sigma}
    $$
  </ul>

  Love the illustration <a href="https://t.ly/KsMH">here</a>.
  <a href="https://t.ly/hEXC">This example</a> is genius.
</section>

<section id="6"><h1>Normalisation</h1>
  Different types of normalisation and when to use each: 
  <a href="https://t.ly/PN_i">Ref</a>.

  <p>
    The relative frequency of events may not be in the range of 0 to 1. 
    But we can make the relative frequency sum to one by normalizing - dividing the 
    probability of each event by the sum of probabilities of all the events.
    A probability is normalized when the probabilities sum to 1.0.
    Any mathematically correct distribution must be normalized, but un-normalized 
    distributions can also be useful if we just want to compare relative probabilities.
  </p>
</section>

<section><h1>Probability distribution</h1>
  A <i>probability distribution</i> is a list of events with their 
  associated probabilities. It is a function that maps from the outcomes to their corresponding probabilities. A <i>distribution</i> in general allows us to ask the question: what is the most likely event?
</section>

<section id="7"><h1>Probability mass function</h1>
  <i>Mass</i> is the quantity of matter in a physical body. Example uses: a mass of cyclists, the film has mass appeal.
  
  <p>
    A probability distribution divides/distributes the whole <i>probability mass</i> (equal to 1) of a random variable across its possible outcomes, the variable's sample space. When the outcomes are discrete, the probability distribution function is called a <code>probability mass function</code>.
  </p>

  <a href="https://t.ly/1iwF">Ref</a>. <a href="https://t.ly/RY3w">Excellent ref</a>.
</section>

<section id="8"><h1>Definitions related to probability</h1>
  <table class="table1 center">
    <tr>
      <th>Term</th>
      <th>Definition</th>
    </tr>
    <tr>
      <td>Event or outcome</td>
      <td>Something that could happen</td>
    </tr>
    <tr>
      <td>Sample space</td>
      <td>List of events</td>
    </tr>
    <tr>
      <td>Discrete sample space</td>
      <td>Discrete set of events or possible outcomes</td>
    </tr>
    <tr>
      <td>Probability</td>
      <td>Relative likelihood or relative frequency of an event occurring</td>
    </tr>
    <tr>
      <td>Normalised PD</td>
      <td>A PD in which all the probabilities sum to 1</td>
    </tr>
    <tr>
      <td>Probability of 0</td>
      <td>The event is guaranteed to not happen</td>
    </tr>
    <tr>
      <td>Probability of 1</td>
      <td>The event is certain to happen</td>
    </tr>
    <tr>
      <td>Conditional probability</td>
      <td>
        The probability of some event given that some other event already 
        occurred. We use the PMF to find this
      </td>
    </tr>

    <tr>
      <td>$p(x)$</td>
      <td>
        The probability of the vehicle being in position $x$.
      </td>
    </tr>

    <tr>
      <td>$p(x,y)$</td>
      <td>
        The probability of the vehicle being in position x and making measurements y.
      </td>
    </tr>

    <tr>
      <td>$p(y|x)$</td>
      <td>
        The probability of making measurements y given that the vehicle is in position x.
      </td>
    </tr>
  </table>
  $$
    P(of\, a\, particular\, event) = 
    \frac{Frequency\, of\, the\, particular\, event\, occurring}
    {Total\, number\, of\, events}
  $$
</section>

<section id="9"><h1>Musings</h1>
  <ul>  
    <li>Probability mass function</li>
      Mass is also a measure of the body's inertia, the resistance to 
      acceleration when a net force is applied.

    <p></p>
    <li>Variance</li>
    $\sigma^2$ is the long-term average ($E$) of the square of the
    deviation from the mean, i.e. $E(X=(x_i-\mu)^2)$. 
    The 'summation' will give the average deviation squared.
    To find $\sigma$, square root is taken over the whole term after
    the summation, which will just be the average deviation.

    <p>
      In the expected value equation, the probability of a random
      variable taking on a particular value is based on prior experiments.
      It doesn't necessarily mean, in a particular set of draws,
      the value will occur a certain number of times. But, if we draw
      enough number of times, we should see the frequency match the 
      probability. For example, if I say the probability of drawing a 
      5 is 0.9. But, I do an experiment of drawing a card only once. 
      It is highly likely that I will draw a 5, but I could also 
      draw another number. But, if I do it enough number of times, 
      5 will be drawn more number of times to match its probability. 
      The expected value will not converge to be the
      mean if the probability isn't $\frac{1}{N}$ for all numbers.
    </p>

    <li>Z-score</li>
    It is similar to mapping 100seconds to the equivalent number of minutes.
    Since a minute is 60s, we divide 100 by 60 to get the answer. Here,
    each x-axis division is a standard deviation (denominator). The numerator
    is the deviation from the mean along the x-axis scale.
  </ul>
</section>

</chapter>

</body>
</html>
