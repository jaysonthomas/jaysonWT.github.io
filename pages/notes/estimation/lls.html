
<!DOCTYPE html>
<html>
<head>
  <title>Linear Least Squares</title>
  <meta name="Linear Least Squares" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../pages/bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Measurement equation</a>
<a href="#1">Deriving The Maximum Likelihood Estimator</a>
<ul class="no-bullets">
  <li><a href="#1.0">Find $p(y|x)$ - collect measurements</a></li>
  <li><a href="#1.1">Find $\hat{x}$ - maximise the probability</a></li>
</ul>
<a href="#2">Example: Fitting A Line With Linear Least Squares</a>
<a href="#3">Relevant code</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Linear Least Squares</h1>
<section id="0"><h1>Measurement equation</h1>
  [<a href="https://t.ly/mwtO">Good quick tutorial</a>].

  <p>
    A linear function of $x$ can be written as: $y = Ax + b$. Consider a system where we take measurements of a state (which is constant at the time of measurement) or a parameter, $x$, such that:
    $$
      \underbrace{y}_{\scriptsize\text{Known}} = 
      \underbrace{H}_{\scriptsize\text{Known} \atop \scriptsize\text{Constant}}
      \underbrace{x}_{\scriptsize\text{Unknown} \atop \scriptsize\text{Constant}} 
      + \underbrace{v}_{\scriptsize\text{Unknown}}
    $$
  </p>

  <ul>
    <li>$x$ is the constant unknown state/parameter vector of length $n$ that is to be estimated.</li>

    <p>
      <li>$y$ is the vector of measurements of length $m$ that is actually received</li>

      <p>
        The measurements are a <n>linear function of the physical quantity perturbed by some unknown noise</n>. We need to have more measurements than unknowns i.e. $m>n$
      </p>
    </p>

    <li>$v$ is the unknown error vector</li>
    <p>
      All we can know are the statistical properties of the noise that can be estimated from the data. The noise is assumed to be drawn from a Gaussian distribution with a $0$ mean and some covariance $R$, i.e. $v$ can be assumed to be some <n>known variance</n> times the identity matrix.
    </p>

    <li>$H$ is the known <i>measurement matrix</i>; also called the <i>measurement model</i></li>
    <p>
      It remains constant when a measurement is being taken. It projects the state/parameter into the <n>measurement space</n>.
    </p>

    $H$ is also sometimes called the <code>Jacobian matrix</code>; it contains the partial derivatives of the measurements with respect to the parameter(s) $x$ of interest. If the jacobian matrix is supposed to tell us what happens to the measurements $y$ for a change in $x$, <m>what happens if $H$ itself is changing with a constant $x$</m>? Does this happen?
  </ul>
  
  <p>
    If the error $v$ is known (we only know the distribution w.r.t. the unknown $x$), we can solve for $x$: $Hx = y - v$. Or if $x$ is known, we can solve for $v$. We don't know either and that's what makes this an estimation problem. A <i>maximum likelihood estimation</i> approach is used which, intuitively, means <n>we look at the measurements and ask what state/parameter is the most likely to yield the particular set of measurements</n>.
  </p>  

  Once we have the measurements $y$, the least squares estimation problem boils down to figuring out the measurement model $H$ which is often the tricky bit.
</section>

<section id="1"><h1>Deriving The Maximum Likelihood Estimator</h1>
  We want to know what state is most consistent with our measurements. Mathematically, the goal is to select an estimate $\hat{x}$ to be the value of $x$ that maximizes the probability of the actual measurements $y$ i.e. choose $\hat{x}$ to maximise $p(y| x = \hat{x})$

  <p>
    There are 2 steps to this process (it is assumed the decision on what $x$ and $H$ are going to represent has been made):
  </p>
  
  <subsection id="1.0"><h1>Find $p(y|x)$ - collect measurements</h1>
    The probability density function $p(y|x)$ in this model, tells us the likelihood of the measurement y when the state is x. We assume that the distribution $p(y|x)$ is Gaussian - we get $Hx$ along with some gaussian noise as the measurment, which means for the same $x$ we could get a different $y$. The parameters of the distribution can be expressed as:

    <ul>
      <li>Mean</li>
      Since $y = Hx + v$ and $v$ is a zero-mean gaussian, the expected value, the mean, is: $\bar{y} = Hx$

      <p>
        <li>Covariance</li>
        Given the properties of our noise model, $v \sim \mathcal{N}(0,R)$, the covariance $(y-\bar{y})(y-\bar{y})^T$ is equal to the covariance of just the noise, $R$.
      </p>
    </ul>

    Thus, $p(y|x) = \mathcal{N}(Hx, R)$. The formula for the <a href="../maths/gaussians.html">Gaussian</a>:

    $$
      p(y|x) = \underbrace{
          \frac{1}{(2\pi)^{\frac{n}{2}}\lvert{R}\rvert^{\frac{1}{2}}}
        }_{\text{Normalizer}}
      e^
        \underbrace{
          {-\frac{1}{2}(y-Hx)^TR^{-1}(y-Hx)}
        }_{\text{Term to maximise}}
    $$
  </subsection>

  <subsection id="1.1"><h1>Find $\hat{x}$ - maximise the probability</h1>
    To find the likelihood of the received measurements, $y$ given $x$ (i.e. the value of the estimate $\hat{x}$ that maximises the likelihood of the actual measurements $y$ given $\hat{x}$), we need to maximise the gaussian function. 
    
    <p>
      The normalizer term does not depend on $x$ at all, i.e. the specific choice of $x$ has no impact on the normalizer and can be ignored. Therefore, we need to maximise the term that's in the exponent; which means we just need to maximise the function in the exponent.
    </p>

    <p>
      To maximise the function, take its derivative, set it equal to $0$ and then solve for $x$ to get the equation for the maximum likelihood estimator $\hat{x}$:
    </p>
    $$
      \hat{x} = (H^TR^{-1}H)^{-1}H^TR^{-1}y
    $$

    <button class="accordion">The derivation</button>
    <div class="panel">
      $$
        -\frac{1}{2}(y-Hx)^TR^{-1}H = 0
      $$
      
      Multiplying both sides by $-2$ to simplify the equation:
      $$
        (y-Hx)^TR^{-1}H = 0
      $$

      Expanding the transpose and rearranging the equation:
      $$\begin{align*}
        (y^T-x^TH^T)R^{-1}H &= 0 \\
        y^TR^{-1}H - x^TH^TR^{-1}H &= 0 \\
        (y^TR^{-1}H - x^TH^TR^{-1}H)^T &= 0^T \\
        (y^TR^{-1}H)^T - (x^TH^TR^{-1}H)^T &= 0 \\
        H^TR^{-T}y - H^T(R^{-1})^THx &= 0 \\
      \end{align*}$$

      Since $R$ is a symmetric matrix ($R=R^T$), $R^{-1}$ is also symmetric: $R^{-1} = (R^{-1})^T$
      $$
        H^TR^{-1}y - H^TR^{-1}Hx = 0 
      $$

      Rearranging:
      $$
        x = (H^TR^{-1}H)^{-1}H^TR^{-1}y
      $$
    </div>

    This is a solution to the least squares estimation problem given some measurements $y$ that are linearly related to some unknown quantity $x$ by the matrix $H$ that we assume we know. Using this, we can solve for the most likely estimate, $\hat{x}$. <n>The best estimate is purely a function of the measurement model $H$ and the actual measurements $y$</n>.
  </subsection>

</section>

<section id="2"><h1>Example: Fitting A Line With Linear Least Squares</h1>
  <div class="container">
    <figure>
      <img style="height:125px; width:auto"
      src="../../../figures/drone/26_leastSquareLineFit.png"/>
      <figcaption>
        Noisy data representing a straight line
      </figcaption>
    </figure>
  </div>
  
  Let's assume that we have data sampled from a straight line $y = at + b$ with some added noise. What we actually want are the properties of the line, which form our unknown state vector $x$:
  $$
    x = \begin{bmatrix}
    a \\ b
    \end{bmatrix}
  $$

  The model assumes that our measurements $y_1, y_2, \cdots$ are drawn according to a linear function $y_i$:
  $$
    y_i = at_i + b + v_i
  $$
  It's the unknown $v_i$ that's perturbing the measurements from the line. The measurements are in the form of:
  $$
    y = Hx + v
  $$

  We have the state vector $x$ and all the measured values: 
  $$\begin{bmatrix}
    \tilde{y_1} \\
    \vdots \\
    \tilde{y_m}
  \end{bmatrix}$$

  The mapping from $x$ to $y$ is given by $H$ that looks as follows:
  $$\begin{bmatrix}
    t_1 & 1 \\
    \vdots & \vdots \\
    t_m & 1
  \end{bmatrix}$$

  Now that we have the measurements and $H$, we can use the equation for the maximum likelihood estimate to find the best estimate for x, $\hat{x}$. 
</section>

<section id="3"><h1>Relevant code</h1>
  <ul>
    <li>Simple state estimation python example - <a href="../../../jlib/docs/estimation/batchlls.html">here</a></li>
  </ul>
</section>
</chapter>

</body>
</html>
