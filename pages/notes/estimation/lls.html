
<!DOCTYPE html>
<html>
<head>
  <title>Linear Least Squares</title>
  <meta name="Linear Least Squares" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>

  <table style="width:100%;" pdf="no"><tr style="width:100%">
    <td style="width:33%;text-align:left;">
      <a class="previous_chapter" href="">Prev Chapter</a>
    </td>
    
    <td style="width:33%;text-align:center;">
      <a href="">Root Chapter</a>
    </td>
    
    <td style="width:33%;text-align:right;">
      <a class="next_chapter" href="">Next Chapter</a>
    </td>
  </tr></table>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Overview</a>
<a href="#1">Deriving The Maximum Likelihood Estimator</a>
<ul class="no-bullets">
  <li><a href="#1.0">Step 1: Find $p(y|x)$</a></li>
  <li><a href="#1.1">Step 2: Find $\hat{x}$</a></li>
</ul>
<a href="#2">Example: Fitting A Line With Linear Least Squares</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Linear Least Squares</h1>
<section id="0"><h1>Overview</h1>
  [<a href="https://t.ly/mwtO">Good quick tutorial</a>].

  <p>
    A linear function of $x$ can be written as: $y = Ax + b$. Consider a system where we take measurements of a state (which is constant at the time of measurement) or a parameter, $x$, such that:
    $$
      \underbrace{\tilde{y}}_{\scriptsize\text{Known}} = 
      \underbrace{H}_{\scriptsize\text{Known} \atop \scriptsize\text{Constant}}
      \underbrace{x}_{\scriptsize\text{Unknown} \atop \scriptsize\text{Constant}} 
      + \underbrace{v}_{\scriptsize\text{Unknown}}
    $$
  </p>

  <ul>
    <li>$x$ is the constant unknown state/parameter vector of length $n$ that is to be estimated.</li>

    <p>
      <li>$\tilde{y}$ is the vector of measurements of length $m$ that is actually received. The measurements are a linear function of the physical quantity perturbed by some unknown noise.</li>
    </p>

    <li>$v$ is the unknown error vector. The noise is assumed to be drawn from a Gaussian distribution with a $0$ mean and some covariance $R$, i.e. $v$ can be assumed to be some known variance times the identity matrix.</li>

    <p>
      <li>$H$ is the <i>measurement matrix</i>. It is assumed to be known. It remains constant when a measurement is being taken. It projects the state/parameter into the measurement space.</li>
    </p>
  </ul>
  
  <p>
    If the error $v$ is known, we can solve for $x$: $Hx = \tilde{y} - v$. Or if $x$ is known, we can solve for $v$. We don't know either and that's what makes this an estimation problem. We use a <i>maximum likelihood estimation</i> approach. <n>Intuitively, we look at the measurements and ask what state/parameter is the most likely to yield the particular set of measurements</n>.
  </p>  

  An important thing to note is that once we have the measurements $\tilde{y}$, the least squares estimation problem boils down to figuring out exactly what the measurement model $H$ looks like. Figuring out how to form $H$ for any estimation problem is often the tricky bit.
</section>

<section id="1"><h1>Deriving The Maximum Likelihood Estimator</h1>
  We want to know what state is most consistent with our measurements. Mathematically, the goal is to select our estimate $\hat{x}$ to be the value of $x$ that maximizes the probability of our actual measurements $\tilde{y}$:
  <br>
  Goal: Choose $\hat{x}$ to maximise $p(y=\tilde{y} | x = \hat{x})$

  <p>
    There are two steps to this process:
  </p>

  <subsection id="1.0"><h1>Step 1: Find $p(y|x)$</h1>
    The probability density function $p(y|x)$ in this model, tells us the likelihood of the measurement y when the state is x. We assume that the distribution $p(y|x)$ is Gaussian. The parameters of the distribution can be expressed as:

    <ul>
      <li>Mean</li>
      Since $y = Hx + v$ and $v$ is a zero-mean gaussian, the expected value, the mean, is: $\bar{y} = Hx$

      <p>
        <li>Covariance</li>
        Given the properties of our noise model, $v \sim \mathcal{N}(0,R)$, the covariance $(y-\bar{y})(y-\bar{y})^T$ is equal to the covariance of just the noise, $R$.
      </p>
    </ul>

    Thus, $p(y|x) = \mathcal{N}(Hx, R)$. The formula for the Gaussian:

    $$
      p(\tilde{y}|x) = \underbrace{
          \frac{1}{(2\pi)^{\frac{n}{2}}\lvert{R}\rvert^{\frac{1}{2}}}
        }_{\text{Normalizer}}
      e^
        \underbrace{
          {-\frac{1}{2}(\tilde{y}-Hx)^TR^{-1}(\tilde{y}-Hx)}
        }_{\text{Term to maximise}}
    $$
  </subsection>

  <subsection id="1.1"><h1>Step 2: Find $\hat{x}$</h1>
    The normalizer term does not depend on $x$ at all. To find the likelihood of the received measurements, $\tilde{y}$ given $x$ (i.e. the value of the estimate $\hat{x}$ that maximises the likelihood of the actual measurements $\tilde{y}$ given $\hat{x}$), we need to maximise the gaussian function. The specific choice of $x$ has no impact on the normalizer and can be ignored. Therefore, we need to maximise the term that's in the exponent; which means we just need to maximise the function in the exponent.

    <p>
      A standard way to solve for the maximum of a function is to find where its derivative is 0. If we take the derivative of the exponent, set it equal to zero, and then solve for $x$, we get the equation for the maximum likelihood estimator $\hat{x}$:
    </p>
    $$
      \hat{x} = (H^TH)^{-1}H^T\tilde{y}
    $$

    This is a solution to the least squares estimation problem given some measurements $\tilde{y}$ that are linearly related to some unknown quantity $x$ by the matrix $H$ that we assume we know. Using this, we can solve for the most likely estimate, $\hat{x}$. <n>The best estimate is purely a function of the measurement model $H$ and the actual measurements $\tilde{y}$</n>.
  </subsection>

</section>

<section id="2"><h1>Example: Fitting A Line With Linear Least Squares</h1>
  <div class="container">
    <figure>
      <img style="height:125px; width:auto"
      src="../../../figures/drone/26_leastSquareLineFit.png"/>
      <figcaption>
        Noisy data representing a straight line
      </figcaption>
    </figure>
  </div>
  
  Let's assume that we have data sampled from a straight line $y = at + b$ with some added noise. What we actually want are the properties of the line, which form our unknown state vector $x$:
  $$
    x = \begin{bmatrix}
    a \\ b
    \end{bmatrix}
  $$

  The model assumes that our measurements $y_1, y_2, \cdots$ are drawn according to a linear function $y_i$:
  $$
    y_i = at_i + b + v_i
  $$
  It's the unknown $v_i$ that's perturbing the measurements from the line. The measurements are in the form of:
  $$
    \tilde{y} = Hx + v
  $$

  We have the state vector $x$ and all the measured values: 
  $$\begin{bmatrix}
    \tilde{y_1} \\
    \vdots \\
    \tilde{y_m}
  \end{bmatrix}$$

  In order to get the vector $y_i$ to match up with the vector $\tilde{y}$, we need an $H$ that looks as follows:
  $$\begin{bmatrix}
    t_1 & 1 \\
    \vdots & \vdots \\
    t_m & 1
  \end{bmatrix}$$

  Now that we have the measurements and $H$, we can use the equation for the maximum likelihood estimate to find the best estimate for x, $\hat{x}$. 
</section>
</chapter>

</body>
</html>
