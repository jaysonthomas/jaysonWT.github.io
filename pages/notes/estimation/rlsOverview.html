
<!DOCTYPE html>
<html>
<head>
  <title>Recursive least squares</title>
  <meta name="Recursive least squares" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../pages/bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Key points</a>
<ul class="no-bullets">
  <li><a href="#0.0">Recursive incremental updates</a></li>
  <li><a href="#0.1">Miscellaneous</a></li>
</ul>
<a href="#1">Computing the estimator gain matrix $K_k$</a>
<a href="#2">RLS algorithm</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Recursive least squares</h1>

<section id="0"><h1>Key points</h1>
  <subsection id="0.0"><h1>Recursive incremental updates</h1>
    RLS is a recursive linear estimator where in an estimate is updated incrementally as new measurements stream in. 

    <p>
      RLS extends the batch LS solution to work on the fly by:
      <ul>
        <li><n>Keeping a running optimal estimate</n> (the LS solution) of some unknown parameter/state for all measurements collected up to the previous time step.</li>
        <li>
          <n>Updating the optimal estimate</n> given the measurement at the current time step.
        </li>
      </ul>
    </p>

    <subsubsection><h1>Mathematical expression</h1>
      A linear recursive estimate $\hat{x}$, at time $k$, is a linear combination of:
      <ul>
        <li>The previous estimate (best guess so far) and</li>
        <li>The <i>current measurement residual</i> (error difference between expected and the actual measurement) <n>weighted</n> by a gain matrix $K_k$</li>
      </ul>
   
      $$\begin{equation} \label{eq:linearRecursiveFormulation}
        \hat{x}_k = \hat{x}_{k-1} + 
        \underbrace{K_k}_{\text{Estimator} \atop \text{gain matrix}}
        \underbrace{(y_k - H_k\hat{x}_{k-1})}_{\text Innovation}
      \end{equation}$$

      <i>Innovation</i> quantifies how well the current measurement matches the previous best estimate. <n>If it is $0$, the old estimate does not need to be updated</n>.
    </subsubsection>
  </subsection>
  
  <subsection id="0.1"><h1>Miscellaneous</h1>
    <ul>
      <li>RLS minimises the variance of the unknowns at every time step.</li>
      <li>RLS forms the update step of the linear Kalman filter.</li>
    </ul>
  </subsection>
</section>

<section id="1"><h1>Computing the estimator gain matrix $K_k$</h1>
  Instead of minimising the error directly, RLS uses a probabilistic formulation to minimise the <n>expected value</n> of the squared measurement residual i.e. the variance at every time step. The lower the variance, the more certain the estimate becomes.

  <p>
    For a single scalar parameter, the variance at time $k$ is as follows:
  </p>
  $$\begin{align*}
    \mathcal L_{RLS} &= \mathbb{E}[(x_k - \hat{x}_k)^2] \\
    &= \sigma^2_k
  \end{align*}$$

  For <n>multiple $n$ unknown parameters</n>, this is equivalent 
  to minimising the trace of the covariance matrix at time step $k$: 
  
  $$\begin{align*}
    \mathcal L_{RLS} &= \mathbb{E}[(x_{1k} - \hat{x}_{1k})^2
    + \cdots + (x_{nk} - \hat{x}_{nk})^2] \\
    &= Trace(\underbrace{P_k}_{\text Estimator \atop \text covariance})
  \end{align*}$$
  
  Using the linear recursive formulation (\ref{eq:linearRecursiveFormulation}), the covariance matrix $P_k$ can be expressed as a function of $K_k$:

  $$
    P_k = (1-K_kH_k)P_{k-1}(1-K_kH_k)^T + K_kR_kK^T_k
  $$  
  
  By using matrix calculus and taking derivatives, the trace of $P_k$ is minimised when $K_k$ has the following value: (find the full derivation in any standard estimation text - <a href="https://t.ly/tzde">Ref</a>)

  $$
    K_k = P_{k-1}H^T_k(H_kP_{k-1}H^T_k + R_k)^{-1}
  $$
  
  Finally, by using this formulation, the recursive definition for $P_k$ can be further simplified: 

  $$\begin{align*}
    P_k &= P_{k-1} - K_kH_kP_{k-1} \\
    &= (I-K_kH_k)P_{k-1}
  \end{align*}$$
  
  <m>The larger the gain matrix $K$, the smaller the new estimator covariance will be</m>.
</section>

<section id="2"><h1>RLS algorithm</h1>
  <ul>
    <li>Initialise the estimator - the estimate and the covariance matrix</li>

    The initial estimate could come from the first measurement $y_0$ and the covariance could come from technical specifications.

    $$\begin{align*}
      \hat{x}_0 &= \mathbb{E}[x] \\
      P_0 &= \mathbb{E}[(x-\hat{x}_0)(x-\hat{x}_0)^T]
    \end{align*}$$
  
    <li>
      Set up the measurement model, defining the Jacobian and the measurement noise with some mean and covariance matrix $R$
    </li>
    $$
      y_k = H_kx + v_k
    $$

    <li>Update the estimate $\hat{x}_k$ and the covariance $P_k$</li>
    For every measurement $k$:
    $$\begin{align*}
      K_k &= P_{k-1}H^T_k(H_kP_{k-1}H^T_k + R_k)^{-1} \\
      \hat{x}_k &= \hat{x}_{k-1} + K_k(y_k - H_k\hat{x}_{k-1}) \\
      P_k &= (I-K_kH_k)P_{k-1}
    \end{align*}$$
    
    Every time a measurement $y_k$ is recorded, the measurement gain $K_k$ is computed and used to update the estimate and the estimator covariance or uncertainty $P_k$. Every time a new measurement is taken, the uncertainty should shrink. 
  </ul>
</section>

</chapter>

</body>
</html>
